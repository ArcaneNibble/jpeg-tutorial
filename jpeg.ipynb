{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An infodump on JPEG compression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python libraries which we will need later\n",
    "\n",
    "from collections import namedtuple\n",
    "from math import pi, cos, sin, sqrt\n",
    "from PIL import Image\n",
    "import struct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These libraries are used for various explanations and demos\n",
    "# They are *NOT* required for any of the actual JPEG code itself\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will be useful later\n",
    "def divroundup(num, divisor):\n",
    "    return (num + divisor - 1) // divisor\n",
    "\n",
    "def clamp(val):\n",
    "    val = round(val)\n",
    "    if val < 0:\n",
    "        return 0\n",
    "    if val > 255:\n",
    "        return 255\n",
    "    return val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preface\n",
    "\n",
    "A while back, I happened to come across the following meme on fedi:\n",
    "\n",
    "<img src=\"imgs/fedi-meme.png\" width=\"500\" alt=\"Screenshot of a meme by @retr0id@retr0.id, featuring a variation on the &quot;how to draw an owl&quot; meme. The first step says &quot;Get some of these thingies&quot; and features a grid of DCT basis functions.\"/>\n",
    "\n",
    "Heh, humorous. Especially if you *do* actually know how JPEG works.\n",
    "\n",
    "But wait, how many people *actually* know how JPEG works? Not just in theory but in enough detail to build a working JPEG decoder and encoder? In this notebook, I hope to explain it in enough detail to not just be able to _implement_ JPEG (since there are plenty of existing software libraries for that) but to understand the \"how and why\" of how JPEG works.\n",
    "\n",
    "This article is being written at the beginning of the year 2025, at a time when social structures around the world are... struggling. Technological change has played a massive role in this (for both \"good\" as well as \"bad\"!), and, although other technologists, activists, and other stakeholders all have their own ideas about what should be done about this situation, I the author believe that a powerful tool to wield against current trends is to *understand* how technological systems work. Take them apart, layer by layer. See what is hiding under the hood. Perhaps use this resulting knowledge to design newer, better systems, or perhaps use it to inform other work.\n",
    "\n",
    "With that, let's encode the rest of the fucking owl!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The JPEG specification\n",
    "\n",
    "Before we try to encode our own JPEGs, we will start by trying to decode one. For this exercise, we will be working with the following image of London Pride by MangakaMaiden Photography, obtained [from Wikimedia Commons](https://commons.wikimedia.org/wiki/File:London_Pride_2023_(53079352061).jpg):\n",
    "\n",
    "<img src=\"imgs/London_Pride_2023_(53079352061).jpg\" width=\"1000\" alt=\"photo of London Pride\"/>\n",
    "\n",
    "In order to be able to decode a JPEG, the first thing we are going to need is the _specification_ defining how the format works. This standard is called either **ISO/IEC 10918-1** or **ITU-T Recommendation T.81**. Here we run into our first tiny hurdle: due to the way certain standards bodies operate, access to this standard requires a payment of 216 CHF if we attempt to obtain it directly from those organizations.\n",
    "\n",
    "Fortunately, not every standards organization works this way, and restricting the spread of information this manner is not aligned with certain ideological trends on the Web, so the specification can instead be had from the W3C's website [here](https://www.w3.org/Graphics/JPEG/itu-t81.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to read a specification\n",
    "\n",
    "The first thing that I usually do when confronted with a specification is to start with the table of contents, followed by very quickly skimming the rest of the document. In this case, we can see that the \"main\" part of the specification is actually very short, only about twenty pages! Although there is important information here, most of the actual details are specified in the various annexes.\n",
    "\n",
    "Section 3, \"Definitions, abbreviations and symbols\" contains, well, exactly what it says it contains. When reading a formal specification, it is _important_ to at least remain aware of these definitions. Formal specification documents often use specialized terminology and jargon in order to refer to ideas in a concise way, and the usage doesn't always correspond with common definitions of words (e.g. 3.1.111 \"run\").\n",
    "\n",
    "The remainder of the \"main\" part, especially section 4 \"General\", gives a high-level overview of how JPEG works and how and in what order the various steps (which are explained in the annexes) are put together. If we read through it, we find that JPEG is actually quite complicated! There are many different choices that can be made during the encoding process and multiple _modes of operation_ (section 4.5) which can be used.\n",
    "\n",
    "Finally, each of the annexes explains one particular operation or sub-step in detail. We will reference each of them as needed.\n",
    "\n",
    "A reasonable question one might ask would be \"how little can we get away with implementing and have it still work?\" After all, technical standards are only words on a page at the end and don't intrinsically come with any enforcement mechanisms (see related: \"MUST (BUT WE KNOW YOU WON'T)\" in the April Fool's [RFC 6919](https://datatracker.ietf.org/doc/html/rfc6919#page-3)). As this notebook is just a proof-of-concept, we are only going to implement the bare essentials needed for this particular image. This code will only handle a _baseline DCT_ image with _interleaved_ components in a single _scan_. (This is the most common way to encode a JPEG, but actually determining that our example image is encoded in this way requires decoding some basic metadata first.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoding the very-computer-y bits\n",
    "\n",
    "Even though the JPEG specification requires a bunch of mathematics and more-theory-laden algorithms, file formats typically also contain \"generic computer-y\" information. What I mean by this phrase is \"information, often metadata, which is encoded in ways that are common across many different file formats.\" This would include information such as the image width and height or data lengths, encoded using bytes.\n",
    "\n",
    "Manipulating the \"generic computer-y\" information in a JPEG file requires basic familiarity with working with binary files and data representations. Working with binary files is outside the scope of the current notebook, but we will be using Python's [`struct` module](https://docs.python.org/3/library/struct.html) to handle it.\n",
    "\n",
    "The relevant part of the JPEG specification we will need for this is Annex B, which explains how a JPEG file is broken up by _markers_.\n",
    "\n",
    "### Parsing markers\n",
    "\n",
    "JPEG _markers_ are listed in Table B.1. Each marker has a \"code assignment\" (i.e. bytes which will be found in the file), a shorthand symbol, and a description. Many of these markers are associated with some information, and this is called a _marker segment_. These segments all store the length of their associated data, as explained in B.1.1.4. The \"actual image data\" is stored as _entropy-coded data_, which is handled differently and does not contain a length. Entropy-coded data is encoded with _byte stuffing_ so that it can never contain a marker, which means that the end of the entropy-coded data occurs when a marker is seen.\n",
    "\n",
    "As a diagram, a JPEG file looks something like this:\n",
    "```text\n",
    "+---------------------------+\n",
    "| SOI marker (0xff 0xd8)    |\n",
    "+---------------------------+   \\                                                               \\\n",
    "| APPn marker (0xff 0xe_)   |   |                                                               |\n",
    "+---------------------------+   |- APPn marker segment                                          |\n",
    "| APPn metadata contents    |   |                                                               |\n",
    "+---------------------------+   X                                                               |\n",
    "| other markers (0xff 0x__) |   |                                                               |\n",
    "+---------------------------+   |- other marker segments (e.g. data tables, Start of Frame)     |\n",
    "| marker segment contents   |   |                                                               |- repeated as appropriate/necessary\n",
    "+---------------------------+   X                                                               |\n",
    "| SOS marker (0xff 0xda)    |   |                                                               |\n",
    "+---------------------------+   |                                                               |\n",
    "| scan metadata             |   |- \"actual image data\"                                          |\n",
    "+---------------------------+   |                                                               |\n",
    "| entropy-coded data        |   |                                                               |\n",
    "+---------------------------+   /                                                               /\n",
    "| EOI marker (0xff 0xd9)    |\n",
    "+---------------------------+\n",
    "```\n",
    "\n",
    "The precise rules for how these segments must be ordered is illustrated in Figure B.16.\n",
    "\n",
    "We can write some code to decode all of this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load file into memory\n",
    "with open('imgs/London_Pride_2023_(53079352061).jpg', 'rb') as f:\n",
    "    jpeg_data = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a very minimal parser for single-scan, baseline DCT JPEG images\n",
    "\n",
    "STANDALONE_MARKERS = {\n",
    "    0xffd8:     \"SOI\",\n",
    "    0xffd9:     \"EOI\",\n",
    "}\n",
    "\n",
    "SEGMENT_MARKERS = {\n",
    "    0xffc0:     \"SOF0\",\n",
    "    0xffc4:     \"DHT\",\n",
    "    0xffdb:     \"DQT\",\n",
    "    0xffdd:     \"DRI\",\n",
    "}\n",
    "\n",
    "JpegSegment = namedtuple('JpegSegment', ['marker', 'data'])\n",
    "JpegImage = namedtuple('JpegImage', ['marker_segs', 'scan_hdr', 'scan_data'])\n",
    "\n",
    "def parse_jpeg_segments(data):\n",
    "    got_a_scan = False\n",
    "    segs = []\n",
    "    # store a list of scan data, separated at restart intervals\n",
    "    scan_data = []\n",
    "\n",
    "    offset = 0\n",
    "    while True:\n",
    "        # try to read a marker\n",
    "        (marker,) = struct.unpack(\">H\", data[offset:offset+2])\n",
    "        if marker == 0xffff:\n",
    "            # there is a fill byte here, which we skip\n",
    "            offset += 1\n",
    "        elif marker in STANDALONE_MARKERS:\n",
    "            # these markers are labeled with a * and do not contain data\n",
    "            marker_human_name = STANDALONE_MARKERS[marker]\n",
    "            print(f\"got {marker_human_name} @ 0x{offset:08x}\")\n",
    "            offset += 2\n",
    "        elif marker in SEGMENT_MARKERS:\n",
    "            # these markers contain a length and then data\n",
    "            marker_human_name = SEGMENT_MARKERS[marker]\n",
    "            (len_,) = struct.unpack(\">H\", data[offset+2:offset+4])\n",
    "            print(f\"got {marker_human_name} of length 0x{len_:04x} @ 0x{offset:08x}\")\n",
    "            payload = data[offset+4:offset+2+len_]\n",
    "            segs.append(JpegSegment(marker, payload))\n",
    "            offset += 2 + len_\n",
    "        elif marker in range(0xffe0, 0xfff0):\n",
    "            # these markers *also* contain a length and then data,\n",
    "            # but APPn is specifically used for metadata which we are ignoring\n",
    "            (len_,) = struct.unpack(\">H\", data[offset+2:offset+4])\n",
    "            print(f\"got APP{marker-0xffe0} of length 0x{len_:04x} @ 0x{offset:08x}\")\n",
    "            payload = data[offset+4:offset+2+len_]\n",
    "            segs.append(JpegSegment(marker, payload))\n",
    "            offset += 2 + len_\n",
    "        elif marker == 0xffda:\n",
    "            print(f\"got SOS @ 0x{offset:08x}\")\n",
    "            assert not got_a_scan, \"multi-scan not supported\"\n",
    "            got_a_scan = True\n",
    "\n",
    "            # scan header, which contains a length\n",
    "            (sos_hdr_len,) = struct.unpack(\">H\", data[offset+2:offset+4])\n",
    "            sos_hdr = data[offset+4:offset+2+sos_hdr_len]\n",
    "\n",
    "            # this now contains entropy-coded data\n",
    "            offset += 2 + sos_hdr_len\n",
    "            scan_data_i_start_off = offset\n",
    "            while offset < len(data):\n",
    "                if data[offset] != 0xff:\n",
    "                    # normal data\n",
    "                    offset += 1\n",
    "                else:\n",
    "                    # either a marker or _byte stuffing_\n",
    "                    if data[offset+1] == 0x00:\n",
    "                        # skip over byte stuffing for now\n",
    "                        offset += 2\n",
    "                    else:\n",
    "                        # skip over fill bytes\n",
    "                        while data[offset+1] == 0xff:\n",
    "                            offset += 1\n",
    "                        \n",
    "                        if data[offset+1] in range(0xd0, 0xd8):\n",
    "                            # print(f\"got RST{data[offset+1] - 0xd0} @ 0x{offset:08x}\")\n",
    "                            scan_data.append(data[scan_data_i_start_off:offset])\n",
    "                            offset += 2\n",
    "                            scan_data_i_start_off = offset\n",
    "                        else:\n",
    "                            break\n",
    "            # last scan\n",
    "            scan_data.append(data[scan_data_i_start_off:offset])\n",
    "        else:\n",
    "            print(f\"unknown marker 0x{marker:04x} @ 0x{offset:08x}\")\n",
    "            raise NotImplementedError\n",
    "\n",
    "        if marker == 0xffd9:\n",
    "            # break on EOI\n",
    "            break\n",
    "    \n",
    "    return JpegImage(segs, sos_hdr, scan_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_jpeg = parse_jpeg_segments(jpeg_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this output, there are a few things to notice:\n",
    "\n",
    "First of all, there are a number of `APPn` \"Reserved for application segments\" in this example file. These segments are used to store metadata which is outside of the scope of the JPEG standard. For example, the first `APP1` segment stores [Exif](https://en.wikipedia.org/wiki/Exif) data and includes information about the camera used to take the photo (a Nikon D5300 in this case). Other segments store Adobe Photoshop metadata, Adobe XMP, and a [ICC profile](https://en.wikipedia.org/wiki/ICC_profile) describing how to interpret the colors in the image.\n",
    "\n",
    "We will not be parsing any of this metadata in our proof-of-concept, but it is important to note that this metadata can result in unintentional privacy breaches. Some online services automatically strip out very sensitive information such as GPS coordinates, but this should not be relied upon in all cases.\n",
    "\n",
    "Second of all, the presence of the `SOF0` marker (as opposed to any other `SOFn`) confirms that we have a baseline DCT image, and the presence of only a single `SOS` marker confirms that the information is stored as a single scan."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frame and scan parameters\n",
    "\n",
    "Metadata which *is* within the scope of the JPEG specification is stored in the `SOF0` and `SOS` segments. Their data fields are specified in B.2.2 and B.2.3 respectively. Let's print them out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "JpegComponentInfo = namedtuple('JpegComponentInfo', ['quant_idx', 'dc_idx', 'ac_idx'])\n",
    "JpegSof0SosInfo = namedtuple('JpegSof0SosInfo', ['x', 'y', 'component_info'])\n",
    "\n",
    "def parse_sof_sos(jpeg):\n",
    "    # decode SOF0\n",
    "    quant_table_map = {}\n",
    "    for seg in jpeg.marker_segs:\n",
    "        if seg.marker == 0xffc0:\n",
    "            # we have already skipped over the length, Lf\n",
    "            (P, Y, X, Nf) = struct.unpack(\">BHHB\", seg.data[:6])\n",
    "            print(f\"~~~~~ SOF0 ~~~~~\")\n",
    "            print(f\"Sample precision (bits per pixel): {P}\")\n",
    "            print(f\"Size: {X} x {Y}\")\n",
    "            print(f\"Number of components: {Nf}\")\n",
    "            for cidx in range(Nf):\n",
    "                component_params = seg.data[6+cidx*3:6+cidx*3+3]\n",
    "                (Ci, HVi, Tqi) = struct.unpack(\">BBB\", component_params)\n",
    "                Hi = HVi >> 4\n",
    "                Vi = HVi & 0xf\n",
    "                print(f\"Component index {cidx}:\")\n",
    "                print(f\"\\tCi identifier: {Ci}\")\n",
    "                print(f\"\\tHori sampling factor: {Hi}\")\n",
    "                print(f\"\\tVert sampling factor: {Vi}\")\n",
    "                print(f\"\\tQuantization table: {Tqi}\")\n",
    "\n",
    "                assert Hi == 1, \"chroma subsampling not supported\"\n",
    "                assert Vi == 1, \"chroma subsampling not supported\"\n",
    "\n",
    "                assert Ci not in quant_table_map, \"duplicate Ci\"\n",
    "                quant_table_map[Ci] = Tqi\n",
    "            \n",
    "            assert P == 8, \"only 8bpp supported\"\n",
    "            assert Nf == 3, \"only 3-component images supported\"\n",
    "    \n",
    "    # decode SOS\n",
    "    # again we have already skipped over Ls\n",
    "    print(f\"~~~~~ SOS ~~~~~\")\n",
    "    Ns = jpeg.scan_hdr[0]\n",
    "    print(f\"Number of components: {Ns}\")\n",
    "    component_info = []\n",
    "    for cidx in range(Ns):\n",
    "        component_params = jpeg.scan_hdr[1+cidx*2:1+cidx*2+2]\n",
    "        Csj = component_params[0]\n",
    "        Tdj = component_params[1] >> 4\n",
    "        Taj = component_params[1] & 0xf\n",
    "        print(f\"Component index {cidx}:\")\n",
    "        print(f\"\\tCi identifier: {Csj}\")\n",
    "        print(f\"\\tDC entropy coding table: {Tdj}\")\n",
    "        print(f\"\\tAC entropy coding table: {Taj}\")\n",
    "\n",
    "        component_info.append(JpegComponentInfo(quant_table_map[Csj], Tdj, Taj))\n",
    "\n",
    "    assert Ns == 3, \"only 3-component scans supported\"\n",
    "    \n",
    "    return JpegSof0SosInfo(X, Y, component_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_sof_sos_info = parse_sof_sos(parsed_jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this information, we now have the dimensions of the image. Our example image happens to be 6000 pixels wide and 4000 pixels tall. There are 8 bits per pixel, and there are three color components per pixel. We also have various indices which associate color components with data tables for \"quantization,\" \"DC entropy coding,\" and \"AC entropy coding.\" These techniques are the core of what makes JPEG actually able to compress images as well as it does.\n",
    "\n",
    "One important bit of information that we are skipping over in this demo are the horizontal and vertical sampling factors. These factors are used for indicating [chroma subsampling](https://en.wikipedia.org/wiki/Chroma_subsampling), a process where color information is stored at a lower resolution compared to the brightness information. We can often get away with doing this because of how the human visual system works, where it is much more sensitive to changes in brightness than it is to changes in color. Support for chroma subsampling is quite common in formats for \"multimedia\" (such as JPEG and Blu-Ray) but is less common in formats for \"computer graphics\" (such as PNG or GIF). For the image we are working with, the sampling factors are all 1 which indicates that chroma subsampling has not been applied (also known as \"4:4:4\" sampling).\n",
    "\n",
    "Something which is explicitly **not** specified in T.81 itself is how to actually interpret the three color components (\"Application-dependent information, e.g. colour space, is outside the scope of this Specification.\") We only know that there are three of them. We will revisit this problem later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantization tables\n",
    "\n",
    "Before we can start decoding, we need to read the data tables referenced by the `SOF0` and `SOS` metadata. These tables are are stored in their corresponding marker segments. We will start with the quantization tables first as they are simpler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_dqt(jpeg):\n",
    "    quant_tables = [None] * 4\n",
    "    for seg in jpeg.marker_segs:\n",
    "        if seg.marker == 0xffdb:\n",
    "            # for 8-bit tables, each table is 65 bytes long\n",
    "            for i in range(len(seg.data) // 65):\n",
    "                Pq = seg.data[i*65] >> 4\n",
    "                Tq = seg.data[i*65] & 0xf\n",
    "                table_i = seg.data[i*65 + 1:(i+1)*65]\n",
    "\n",
    "                print(f\"Got quantization table {Tq}\")\n",
    "                quant_tables[Tq] = table_i\n",
    "                assert Pq == 0, \"only 8-bit quantization tables supported\"\n",
    "    return quant_tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quant_tables = parse_dqt(parsed_jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each quantization table is just an array of 64 numbers!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Huffman tables\n",
    "\n",
    "The next set of tables we need to read are those for DC and AC entropy coding. _Entropy coding_ is a category of techniques for performing _lossless_ data compression, storing information in (hopefully) less space while still allowing exactly the same original data to be recovered at the end. Even though JPEG as a whole (at least in the DCT modes being used here) is a form of _lossy_ compression (the output is not exactly the same as the original data), it still makes use of lossless compression as a sub-step. The supported entropy coding techniques are _Huffman coding_ and _arithmetic coding_. Our image is using Huffman coding.\n",
    "\n",
    "Unlike all of the previous wrangling of bytes and file structures, Huffman coding is the first \"fancy algorithm\" we have run into. We will present some code which parses the JPEG Huffman trees before explaining how it works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store codes as a binary tree\n",
    "def add_codeword_to_tree(tree, code, code_len, code_val):\n",
    "    for biti in range(code_len):\n",
    "        # start at the MSB\n",
    "        bit = 1 if code & (1 << (code_len - 1 - biti)) else 0\n",
    "        \n",
    "        if biti == code_len - 1:\n",
    "            # Leaf\n",
    "            assert tree[bit] is None\n",
    "            tree[bit] = code_val\n",
    "        else:\n",
    "            # Intermediate node\n",
    "            if tree[bit] is None:\n",
    "                tree[bit] = [None, None]\n",
    "            tree = tree[bit]\n",
    "\n",
    "def parse_dht(jpeg):\n",
    "    huff_tables_ac = [None] * 2\n",
    "    huff_tables_dc = [None] * 2\n",
    "    for seg in jpeg.marker_segs:\n",
    "        if seg.marker == 0xffc4:\n",
    "            huff_data = seg.data\n",
    "            while huff_data:\n",
    "                huff_tree = [None, None]\n",
    "                # indices\n",
    "                Tc = huff_data[0] >> 4\n",
    "                Th = huff_data[0] & 0xf\n",
    "                # L_i\n",
    "                num_codewords = huff_data[1:17]\n",
    "                huff_data = huff_data[17:]\n",
    "                print(f\"Got Huffman table {Tc}, {Th}\")\n",
    "\n",
    "                min_code_len = 0\n",
    "                for (i, num) in enumerate(num_codewords):\n",
    "                    if num != 0:\n",
    "                        min_code_len = i + 1\n",
    "                        break\n",
    "                assert min_code_len > 0, \"no codewords!\"\n",
    "\n",
    "                code_wip = 0\n",
    "                # for each code length...\n",
    "                for code_len in range(min_code_len, 17):\n",
    "                    num_codewords_of_this_len = num_codewords[code_len - 1]\n",
    "                    # ...read the list of values\n",
    "                    for _j in range(num_codewords_of_this_len):\n",
    "                        code_val = huff_data[0]\n",
    "                        huff_data = huff_data[1:]\n",
    "                        print(f\"{code_wip:0{code_len}b} = 0x{code_val:02x}\")\n",
    "                        add_codeword_to_tree(huff_tree, code_wip, code_len, code_val)\n",
    "                        code_wip += 1\n",
    "                    code_wip <<= 1\n",
    "\n",
    "                if Tc == 0:\n",
    "                    huff_tables_dc[Th] = huff_tree\n",
    "                elif Tc == 1:\n",
    "                    huff_tables_ac[Th] = huff_tree\n",
    "                else:\n",
    "                    assert False, \"invalid Tc\"\n",
    "\n",
    "    return (huff_tables_dc, huff_tables_ac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "huff_tables = parse_dht(parsed_jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Huffman and prefix codes\n",
    "\n",
    "The fundamental idea behind Huffman coding is that we can make data take up less space if we use fewer bits to refer to the more common data items and more bits to refer to the less common data items. The input data items are called the _source symbols_, the bits that we use to represent them are called _codewords_, and the particular mapping between the data items and the codewords is called a _code_. Huffman _coding_ is one particular technique (an algorithm) for creating codes which are provably \"optimal\" under certain assumptions. (It is not possible for a lossless compression algorithm to be optimal in all cases. [Relevant xkcd](https://xkcd.com/1381/).)\n",
    "\n",
    "Huffman coding generates a _prefix code_, a code where no codeword is a prefix of another codeword. Prefix codes are useful because they can be decoded without ambiguity without having to put any separators between the codewords (_uniquely decodable_).\n",
    "\n",
    "### Examples\n",
    "\n",
    "For example, this is a prefix code:\n",
    "\n",
    "| Data   | Codeword |\n",
    "| ------ | -------- |\n",
    "| `a`    | 01       |\n",
    "| `b`    | 100      |\n",
    "| `c`    | 101      |\n",
    "\n",
    "If we were to assume that each input data item was originally stored using 8 bits (one byte, perhaps no other bytes beyond `a`/`b`/`c` are ever used?), this particular code easily achieves a compression ratio of over 50% because it is able to store each item using only 2-3 bits.\n",
    "\n",
    "If we change up some of the codeword assignments, then the following is no longer a prefix code, because the code for `a` (01) is a prefix of the code for `c` (011):\n",
    "\n",
    "| Data   | Codeword |\n",
    "| ------ | -------- |\n",
    "| `a`    | 01       |\n",
    "| `b`    | 100      |\n",
    "| `c`    | 011      |\n",
    "\n",
    "However, this code is still uniquely decodable! We can show this by coming up with a procedure for decoding it. An ambiguity can potentially arise only when we encounter a 01 input (is it going to be `a` or `c`?). To create a decoding procedure, we will list out all possibilities for the next bits which can follow and then specify what our decoder will do upon encountering each of them:\n",
    "\n",
    "| Input | Decode                                                                             |\n",
    "| ----- | ---------------------------------------------------------------------------------- |\n",
    "| 010   | `a-` (consume 2 bits)                                                              |\n",
    "| 01100 | `ab` (consume 5 bits; there is no valid 00x codeword, so it cannot decode as `c?`) |\n",
    "| 01101 | `c-` (consume 3 bits)                                                              |\n",
    "| 01110 | `cb` (consume 5 bits; the next bit must be 0 or else the input is invalid)         |\n",
    "| 01111 | invalid (no code can start with 11)                                                |\n",
    "\n",
    "In mathematical terms, prefix codes are _sufficient_ but not _necessary_ to be uniquely decodable.\n",
    "\n",
    "However, all of these prefix code examples quite obviously have redundant information. (This can be verified more formally using [Kraft's inequality](https://en.wikipedia.org/wiki/Kraft%E2%80%93McMillan_inequality).) For example, it's possible to do the following:\n",
    "\n",
    "| Data   | Code |\n",
    "| ------ | ---- |\n",
    "| `a`    | 0    |\n",
    "| `b`    | 10   |\n",
    "| `c`    | 11   |\n",
    "\n",
    "This is also a prefix code, and it is shorter than all of the above examples.\n",
    "\n",
    "In contrast this is a not a prefix code, and it is also not uniquely decodable:\n",
    "\n",
    "| Data   | Code |\n",
    "| ------ | ---- |\n",
    "| `a`    | 1    |\n",
    "| `b`    | 11   |\n",
    "| `c`    | 111  |\n",
    "\n",
    "It is not possible to tell whether a long string of 1s, such as `111111` is supposed to be all `a`s, `b`s, `c`s, or any other combination of source symbols without separators.\n",
    "\n",
    "### Huffman as used in JPEG\n",
    "\n",
    "Because of the optimality of Huffman coding, the name \"Huffman code\" is often used to refer to any prefix code regardless of how the code was actually created. When we are _decoding_ as we are now, we don't need to actually understand how to construct a Huffman code or otherwise make a code which minimizes redundant information. We only need to understand how to decode the prefix code stored in the JPEG file we are reading. In order to do this, we need to have the map from codewords back to (source) symbol values. Instead of being listed out explicitly like in the above example tables, this information is encoded in the `DHT` segment in an abbreviated form, and we need to understand how that works.\n",
    "\n",
    "JPEG uses _length-limited canonical Huffman codes_ in order to store this mapping information even more efficiently. A _canonical_ Huffman code is a prefix code where the numerical values of the codewords (i.e. the particular `0`/`1` bits) are generated in a specific way (sorting by length, generating sequential binary values for each code of a given length, and appending 0 bits when the next length is reached). This _canonical_ assignment of codewords allows the table to be represented by only storing the _total number_ of codewords of each particular bit length and the associated source symbols. The actual bits which make up each of the codewords does not have to be stored but are instead reconstructed by following the same algorithm used to generate them.\n",
    "\n",
    "For example, the following code (which is canonical according to this algorithm):\n",
    "\n",
    "| Data   | Code |\n",
    "| ------ | ---- |\n",
    "| `a`    | 0    |\n",
    "| `b`    | 10   |\n",
    "| `c`    | 11   |\n",
    "\n",
    "can instead be stored as the following:\n",
    "\n",
    "| Codeword length | # codewords of this length | Source data symbols list |\n",
    "| --------------- | -------------------------- | ------------------------ |\n",
    "| 1               | 1                          | `a`                      |\n",
    "| 2               | 2                          | `b`, `c`                 |\n",
    "\n",
    "This can be turned back into the full table above by following the specific codeword assignment algorithm to calculate the codewords for length 1, followed by those for length 2 (and so on, for codes with more codewords).\n",
    "\n",
    "### Our implementation\n",
    "\n",
    "In our code, we will store prefix codes as binary trees. Every time an input bit is consumed, the tree is traversed one level until a leaf node is eventually reached containing a source symbol value. This works and is a common \"theoretical\" way of representing prefix codes but is not the most computationally efficient way of doing this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of next steps\n",
    "\n",
    "At this point, we have all of the data tables loaded and are almost ready to start decoding image scans (which contain the \"actual data\" we care about). However, before doing this, we are going to want a high-level overview of what to expect next. This is because information in the JPEG bitstream is \"all mixed up\":\n",
    "\n",
    "```text\n",
    "... 0101100011000111000101010111001010110001011001011111110100101010111000010001000011011100100010000101 ...\n",
    "    \\_/\\__/\\___/\\__/\\__/\\___/\\_/\\____/\\___/\\_/\\__/\\__/\\__/\\___/\\_/\\____/\\_/\\____/ ...\n",
    "     |  |    |   |  ... (more Huffman codewords and coefficient values)\n",
    "     |  |    |   +- AC coefficient value\n",
    "     |  |    +----- AC Huffman codeword (color component 0)\n",
    "     |  +---------- DC coefficient value\n",
    "     +------------- DC Huffman codeword (color component 0)\n",
    "\n",
    "    \\_____/\\_______/\\_______/\\_______/\\______/\\______/\\_______/\\_______/\\_______/ ...\n",
    "       |       |        |        |       |       |        |        |        |\n",
    "       |       +--------+--------+------ | ------+--------+--------+--------+-- AC coefficients\n",
    "       +---------------------------------+------------------------------------- DC coefficients\n",
    "    \\________________________________/\\_________________________________________/ ...\n",
    "                    |                                       |                       |\n",
    "                    |                                       |                       +- 8x8 block for color component 2  \\\n",
    "                    |                                       +------------------------- 8x8 block for color component 1  +--\\\n",
    "                    +----------------------------------------------------------------- 8x8 block for color component 0  /  |\n",
    "                                                                                                                           |\n",
    "                                                               minimum coded unit, repeats until entire image is encoded --/\n",
    "```\n",
    "\n",
    "In addition to being \"all mixed up\" and interleaved, each 8x8 block can contain a different number of \"AC coefficients\" due to _run-length encoding_, and each color component can use a different Huffman code.\n",
    "\n",
    "In order to make sense of all of this, the following is the procedure for _encoding_ a JPEG using the _baseline DCT_ mode:\n",
    "1. Do transformations related to color space and chroma subsampling. This eventually gives us three separate color components. This step is \"more abstracted away\" from the bitstream handling we are about to do, and so we can ignore it and come back to it later.\n",
    "2. Break each color component up into 8x8 pixel blocks.\n",
    "3. Perform a _discrete cosine transform_ (DCT) on each of these blocks. There is a lot of mathematical theory about how the DCT works, but it fundamentally just _changes each block of numbers (64 pixels) into a different block of numbers (64 coefficients)_. The only bit of the theory we need at this point is to know that _one_ of the numbers is referred to as the \"DC\" coefficient, and the remaining 63 are referred to as the \"AC\" coefficients.\n",
    "4. Take the output coefficients and perform _quantization_ on the numbers. This takes all of the numbers and makes them smaller (as well as less accurate due to rounding). This step is _lossy_ and throws away information from the original image. The idea here is to hopefully try to only throw away information which is less perceptible to humans.\n",
    "5. Perform what I will call \"some miscellaneous transformations\" on these blocks of numbers.\n",
    "    1. The one \"DC coefficient\" in each block is stored as the _difference from the previous DC coefficient_ (i.e. the difference between the DC coefficient of the current 8x8 block minus the DC coefficient of the previous 8x8 block). This step is called _differential DC encoding_. The hope is that these differences will tend to be smaller in magnitude compared to the absolute values of the coefficients. This hope is justified based on the interpretation of what the DCT does as well as observations of what \"typical\" image data looks like.\n",
    "    2. The 63 \"AC coefficients\" are rearranged into a zigzag order compared to the \"normal\" order in which the outputs of a DCT would be written. This helps to cluster together the coefficients which contribute to the image in a \"similar\" way (again, this \"similarity\" is explained based on the interpretation of what the DCT does).\n",
    "    3. Perform _run-length encoding_ on any 0s that might appear in the AC coefficients. Run-length encoding replaces _runs_ of 0s (i.e. several 0s in a row) with one single symbol representing \"there are $n$ 0s here.\" The combination of the DCT and quantization steps was designed to try to generate a lot of 0s for this step to encode.\n",
    "6. Put the numbers through the entropy coding sub-step (Huffman coding in this case).\n",
    "    1. The scale (number of bits) of the DC coefficient is encoded using the DC Huffman table for this color component, and then the DC coefficient difference is stored directly (using the minimum number of bits necessary). The differences themselves are _not_ Huffman encoded.\n",
    "    2. The run length (number of 0s which come before this coefficient) and scale of the AC coefficients is packed together and then encoded using the AC Huffman table for this color component. The coefficient value is again stored directly.\n",
    "\n",
    "To restate the result of this procedure once again, the final bitstream in a JPEG image contains a *mix* of Huffman codewords (from two different code tables) and data values (DC coefficient differences, AC coefficients). The exact number of values can also vary across 8x8 blocks due to run-length encoding. Even though decoding is the opposite of encoding, because all of these different pieces are all mixed together, the separation between the steps isn't very \"clean\". In this demo, we will be handling many of the steps all at once.\n",
    "\n",
    "One final tiny piece of information we need is the _restart interval_, the number of blocks after which certain state (in this case, the DC difference value) is reset. This is stored by itself in a marker segment.\n",
    "\n",
    "_Aside:_ The above explanation has not made a clear distinction between 8x8 pixel blocks and _minimum coded units_. This distinction is important when chroma subsampling is used, but it is much less critical for our example where the chroma is not subsampled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_restart_interval(jpeg):\n",
    "    rst_interval = -1\n",
    "    for seg in jpeg.marker_segs:\n",
    "        if seg.marker == 0xffdd:\n",
    "            rst_interval = struct.unpack(\">H\", seg.data)[0]\n",
    "    return rst_interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "restart_interval = get_restart_interval(parsed_jpeg)\n",
    "print(f\"Restart interval: {restart_interval} MCUs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actually decoding scan data\n",
    "\n",
    "The following code helps with processing the bitstream bit-by-bit (i.e. data does not align with byte boundaries):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bit manip helpers\n",
    "\n",
    "def get_bit(data, pos):\n",
    "    (byte_pos, bit_pos) = pos\n",
    "    bit_val = 1 if data[byte_pos] & (1 << (7 - bit_pos)) else 0\n",
    "    if bit_pos != 7:\n",
    "        bit_pos += 1\n",
    "    else:\n",
    "        bit_pos = 0\n",
    "        byte_pos += 1\n",
    "    return (bit_val, (byte_pos, bit_pos))\n",
    "\n",
    "def get_bits(data, pos, nbits):\n",
    "    val = 0\n",
    "    for _i in range(nbits):\n",
    "        (b, pos) = get_bit(data, pos)\n",
    "        val = (val << 1) | b\n",
    "    return (val, pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code pulls one \"thing\" from the bitstream, either a Huffman codeword or a coefficient. It pulls exactly as many bits as are required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# traverse the Huffman table to get one symbol\n",
    "def decode_huff(data, pos, huff_table):\n",
    "    while True:\n",
    "        (b, pos) = get_bit(data, pos)\n",
    "        huff_table = huff_table[b]\n",
    "        if isinstance(huff_table, int):\n",
    "            return (huff_table, pos)\n",
    "\n",
    "# grab a group of bits for a coefficient, converting negative numbers back\n",
    "# see F.1.2.1.1\n",
    "def decode_coeff(data, pos, nbits):\n",
    "    if nbits == 0:\n",
    "        return (0, pos)\n",
    "    \n",
    "    (val, pos) = get_bits(data, pos, nbits)\n",
    "    if val & (1 << (nbits - 1)):\n",
    "        # msb is 1 --> positive value\n",
    "        return (val, pos)\n",
    "    else:\n",
    "        # negative value\n",
    "        val = -(1 << nbits) + val + 1\n",
    "        return (val, pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code handles Huffman decompression, run-length decompression of AC coefficients, and decoding the differential DC encoding. This is all done at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decode one 8x8 block, known as a \"data unit\"\n",
    "# see F.1.2.1 and F.1.2.2\n",
    "def decode_block(data, pos, dc_huff_table, ac_huff_table, pred=0):\n",
    "    # initialize output to all 0s\n",
    "    coeffs = [0] * 64\n",
    "\n",
    "    # decode DC difference\n",
    "    (dc_sym, pos) = decode_huff(data, pos, dc_huff_table)\n",
    "    assert dc_sym in range(0, 12), \"invalid DC coeff scale\"\n",
    "    (dc_diff, pos) = decode_coeff(data, pos, dc_sym)\n",
    "    coeffs[0] = dc_diff + pred\n",
    "\n",
    "    # decode AC coefficients, including runs of 0s\n",
    "    ac_i = 1\n",
    "    while ac_i < 64:\n",
    "        (ac_sym, pos) = decode_huff(data, pos, ac_huff_table)\n",
    "        if ac_sym == 0xf0:\n",
    "            # 16 zeros\n",
    "            ac_i += 16\n",
    "        elif ac_sym == 0x00:\n",
    "            # end-of-block marker\n",
    "            break\n",
    "        else:\n",
    "            rrrr = ac_sym >> 4\n",
    "            ssss = ac_sym & 0xf\n",
    "            assert ssss in range(1, 11), \"invalid AC coeff scale\"\n",
    "            # a run of rrrr zeros\n",
    "            ac_i += rrrr\n",
    "            # followed by one AC coefficient\n",
    "            (coeffs[ac_i], pos) = decode_coeff(data, pos, ssss)\n",
    "            ac_i += 1\n",
    "    \n",
    "    return (coeffs, pos)\n",
    "\n",
    "# decode one MCU, consisting of a data unit for each component (interleaved)\n",
    "# see E.2.5\n",
    "def decode_mcu(data, pos, sof_sos_info, dc_huff_tables, ac_huff_tables, preds=[0, 0, 0]):\n",
    "    coeffs = [None] * 3\n",
    "    for component_i in range(3):\n",
    "        dc_huff = dc_huff_tables[sof_sos_info.component_info[component_i].dc_idx]\n",
    "        ac_huff = ac_huff_tables[sof_sos_info.component_info[component_i].ac_idx]\n",
    "\n",
    "        (coeffs[component_i], pos) = decode_block(data, pos, dc_huff, ac_huff, preds[component_i])\n",
    "        # make sure to update the DC predictor after decoding a block\n",
    "        preds[component_i] = coeffs[component_i][0]\n",
    "    \n",
    "    return (coeffs, pos)\n",
    "\n",
    "# decode a restart interval\n",
    "# see E.2.4\n",
    "def decode_restart_interval(data, sof_sos_info, dc_huff_tables, ac_huff_tables, restart_interval=-1):\n",
    "    # if there is no restart interval, calculate how many MCUs are in the image\n",
    "    if restart_interval == -1:\n",
    "        mcu_w = divroundup(sof_sos_info.x, 8)\n",
    "        mcu_h = divroundup(sof_sos_info.y, 8)\n",
    "        restart_interval = mcu_w * mcu_h\n",
    "\n",
    "    # remove byte stuffing (see F.1.2.3)\n",
    "    data_ = b''\n",
    "    i = 0\n",
    "    while i < len(data):\n",
    "        if data[i] != 0xff:\n",
    "            data_ += data[i:i+1]\n",
    "            i += 1\n",
    "        else:\n",
    "            if data[i+1] == 0x00:\n",
    "                # remove stuffing\n",
    "                data_ += b'\\xff'\n",
    "                i += 2\n",
    "            else:\n",
    "                # we must be at the end of the data\n",
    "                for j in range(i, len(data)):\n",
    "                    assert data[j] == 0xff\n",
    "    data = data_\n",
    "\n",
    "    # output\n",
    "    mcus = [None] * restart_interval\n",
    "    # current position\n",
    "    pos = (0, 0)\n",
    "    # reset DC predictors\n",
    "    preds = [0, 0, 0]\n",
    "\n",
    "    for i in range(restart_interval):\n",
    "        (mcus[i], pos) = decode_mcu(data, pos, sof_sos_info, dc_huff_tables, ac_huff_tables, preds)\n",
    "    \n",
    "    # for debugging, check to make sure all the trailing bits are 1\n",
    "    if pos[0] != len(data):\n",
    "        assert pos[0] == len(data) - 1\n",
    "        last_byte = data[pos[0]]\n",
    "        for biti in range(pos[1], 8):\n",
    "            assert last_byte & (1 << (7 - biti))\n",
    "    \n",
    "    return mcus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_jpeg_scan(scan_data, sof_sos_info, dc_huff_tables, ac_huff_tables, restart_interval=-1):\n",
    "    mcus = []\n",
    "    for i in range(len(scan_data)):\n",
    "        # print(f\"Decoding {i+1}/{len(scan_data)}\")\n",
    "        this_mcus = decode_restart_interval(scan_data[i], sof_sos_info, dc_huff_tables, ac_huff_tables, restart_interval)\n",
    "        mcus += this_mcus\n",
    "    print(f\"Decoded {len(mcus)} MCUs in total\")\n",
    "    return mcus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decode_jpeg_mcus = decode_jpeg_scan(parsed_jpeg.scan_data, parsed_sof_sos_info, huff_tables[0], huff_tables[1], restart_interval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, this implementation certainly isn't winning any benchmarks. However, we *have* successfully decoded all of the MCUs. This means that all of the steps related to \"entropy coding\" have been taken care of!\n",
    "\n",
    "### Dequantization\n",
    "\n",
    "The next step in the decoding process is to undo the _quantization_ procedure. This is spelled out in A.3.4, but it simply boils down to... multiplying each coefficient by the corresponding value in the _quantization tables_. Since both the coefficients and the quantization table are currently still in zigzag order, we can do this step *without* any shuffling or rearranging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dequantize_jpeg(mcus, sof_sos_info, quant_tables):\n",
    "    for mcu_i in range(len(mcus)):\n",
    "        for component_i in range(3):\n",
    "            q_table = quant_tables[sof_sos_info.component_info[component_i].quant_idx]\n",
    "            for coeff_i in range(64):\n",
    "                mcus[mcu_i][component_i][coeff_i] *= q_table[coeff_i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "dequantize_jpeg(decode_jpeg_mcus, parsed_sof_sos_info, quant_tables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we have dequantized data that is ready to go into the inverse DCT transformation.\n",
    "\n",
    "But... is there any quick and dirty way to check our work along the way? It feels like we should be at a natural abstraction boundary, as we have just turned a stream of bits into nice blocks of numbers. It would certainly not be fun to wade through all of the remaining calculations just to end up with the wrong output with no clue where the errors could have come from.\n",
    "\n",
    "In fact, there is a way to check! However, the explanation of _why_ this quick and dirty trick works requires a deeper understanding of the DCT which will come later (we are plotting just the DC terms of the luma component, giving us a downscaled version of the original image)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo_quick_check_dequantize():\n",
    "    ylist = []\n",
    "    h_mcus = divroundup(parsed_sof_sos_info.y, 8)\n",
    "    w_mcus = divroundup(parsed_sof_sos_info.x, 8)\n",
    "    for y in range(h_mcus):\n",
    "        xlist = []\n",
    "        for x in range(w_mcus):\n",
    "            xlist.append(decode_jpeg_mcus[y * w_mcus + x][0][0])\n",
    "        ylist.append(xlist)\n",
    "    plt.pcolormesh(ylist[::-1])\n",
    "demo_quick_check_dequantize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The inverse DCT, as naively as possible\n",
    "\n",
    "We are finally at the point where we have to deal with inverting the _discrete cosine transform_ (DCT). Formulas are given in section A.3.3.\n",
    "\n",
    "We also handle zigzag reordering at the same time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "DCT_BASIS_IDX_TO_ZIGZAG = [\n",
    "    0, 1, 5, 6, 14, 15, 27, 28,\n",
    "    2, 4, 7, 13, 16, 26, 29, 42,\n",
    "    3, 8, 12, 17, 25, 30, 41, 43,\n",
    "    9, 11, 18, 24, 31, 40, 44, 53,\n",
    "    10, 19, 23, 32, 39, 45, 52, 54,\n",
    "    20, 22, 33, 38, 46, 51, 55, 60,\n",
    "    21, 34, 37, 47, 50, 56, 59, 61,\n",
    "    35, 36, 48, 49, 57, 58, 62, 63,\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the first attempt, we can try to copy the formula as directly as possible:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the IDCT on an 8x8 data unit, blindly according to the formula\n",
    "def idct_block(output, mcu_x, mcu_y, mcu_w, coeffs):\n",
    "    px_xbase = mcu_x * 8\n",
    "    px_ybase = mcu_y * 8\n",
    "    px_w = mcu_w * 8\n",
    "\n",
    "    for y in range(8):\n",
    "        for x in range(8):\n",
    "            s_yx = 0\n",
    "            # summation symbols are just for loops\n",
    "            for v in range(8):\n",
    "                for u in range(8):\n",
    "                    # constant\n",
    "                    if u == 0 and v == 0:\n",
    "                        # 1/4 * 1/sqrt(2) * 1/sqrt(2) = 1/4 * 1/2 = 1/8\n",
    "                        a = 1 / 8\n",
    "                    elif u == 0 or v == 0:\n",
    "                        # 1/4 * 1/sqrt(2) * 1\n",
    "                        a = 1 / (4 * sqrt(2))\n",
    "                    else:\n",
    "                        a = 1 / 4\n",
    "\n",
    "                    # undo the zigzag ordering (implicitly, without having to store the unscrambled array)\n",
    "                    S_vu = coeffs[DCT_BASIS_IDX_TO_ZIGZAG[v * 8 + u]]\n",
    "\n",
    "                    s_yx += a * S_vu * cos(((2*x+1)*u/16)*pi) * cos(((2*y+1)*v/16)*pi)\n",
    "\n",
    "            output[(px_ybase + y) * px_w + (px_xbase + x)] = s_yx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the above code does technically work, attempting to run it will take an unreasonable amount of time to complete. (You can test this by running the above code cell to replace the `idct_block` function and then rerunning the cell below which computes `decoded_jpeg_components`.)\n",
    "\n",
    "In order to both speed it up and later experiment with some interesting mathematical properties of the DCT, we are going to take advantage of the _linearity_ of the DCT.\n",
    "\n",
    "### Linearity\n",
    "\n",
    "A function is called _linear_ if:\n",
    "1. for any two inputs $x$ and $y$, $f(x+y) = f(x) + f(y)$. This property is called _additivity_.\n",
    "2. for any input $x$ and scale factor $a$, $f(ax) = af(x)$. This property is called _homogeneity_.\n",
    "\n",
    "_Note:_ This meaning of linearity which is being used here (_linear maps_) does *not* refer to _linear polynomials_ of the form $y=mx+b$ which are typically taught in the compulsory education system.\n",
    "\n",
    "A linear function has a very useful property of obeying the _superposition principle_. This principle means that the _same_ result will be obtained from putting some input into a function as would be obtained by breaking apart that input (into pieces which sum together), applying the function to each piece individually, and then adding together those smaller results.\n",
    "\n",
    "### Why does linearity matter?\n",
    "\n",
    "The input that we want to put in to the IDCT operation is a set of 64 coefficients (denoted symbolically as $S_{vu}$). Instead of having to compute the entire formula involving summing up cosines over and over again, we can apply linearity and the superposition principle as follows:\n",
    "\n",
    "1. Precompute the IDCT for the case when $S_{00} = 1$ and every other $S_{vu} = 0$. This gives us an output $s_{yx}$ which we can store as an 8x8 array of numbers.\n",
    "2. Precompute the IDCT for the case when $S_{01} = 1$ and every other $S_{vu} = 0$. This gives us an output $s_{yx}$ which we can store as an 8x8 array of numbers.\n",
    "3. Repeat the computation for every combination of $S_{vu}$, which gives us an 8x8 array where each item is itself an 8x8 array of numbers.\n",
    "4. For each block of actual data we want to compute the IDCT of:\n",
    "    1. Initialize a temporary 8x8 array of numbers to all 0s\n",
    "    2. For element $S_{00}$ of the actual data input, look for the precomputed data where $S_{00} = 1$ and every other $S_{vu} = 0$. Scale this precomputed data by the value of $S_{00}$ in the actual data input (relying on the homogeneity property), and then add it to the temporary array (relying on the additivity property).\n",
    "    3. Repeat for every element of the input.\n",
    "\n",
    "These precomputed values can be called the IDCT \"basis functions.\"\n",
    "\n",
    "### In code?\n",
    "\n",
    "Instead of proving the linearity property with mathematical symbols (a reasonably-common exercise which can be found in many signal processing textbooks), we are going to instead perform refactoring on the above code until the linearity property becomes readily apparent.\n",
    "\n",
    "To rephrase, our goal right now is to take the above code and refactor it so that it is somehow faster. One thing we can notice is that there are only $8*8*8*8$ possible values for the $constant * cosine_1 * cosine_2$ part of the calculation, because each of `x`/`y`/`u`/`v` only ever range from $[0, 8)$. We can try precomputing them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precompute_idct():\n",
    "    result = []\n",
    "    for y in range(8):\n",
    "        for x in range(8):\n",
    "            for v in range(8):\n",
    "                for u in range(8):\n",
    "                    # constant\n",
    "                    if u == 0 and v == 0:\n",
    "                        # 1/4 * 1/sqrt(2) * 1/sqrt(2) = 1/4 * 1/2 = 1/8\n",
    "                        a = 1 / 8\n",
    "                    elif u == 0 or v == 0:\n",
    "                        # 1/4 * 1/sqrt(2) * 1\n",
    "                        a = 1 / (4 * sqrt(2))\n",
    "                    else:\n",
    "                        a = 1 / 4\n",
    "                    \n",
    "                    # constant * cosine_1 * cosine_2\n",
    "                    val = a * cos(((2*x+1)*u/16)*pi) * cos(((2*y+1)*v/16)*pi)\n",
    "\n",
    "                    result.append(val)\n",
    "    return result\n",
    "idct_demo_precompute = precompute_idct()\n",
    "\n",
    "# compute the IDCT on an 8x8 data unit, with some precomputation\n",
    "def idct_block(output, mcu_x, mcu_y, mcu_w, coeffs):\n",
    "    px_xbase = mcu_x * 8\n",
    "    px_ybase = mcu_y * 8\n",
    "    px_w = mcu_w * 8\n",
    "\n",
    "    for y in range(8):\n",
    "        for x in range(8):\n",
    "            s_yx = 0\n",
    "            # summation symbols are just for loops\n",
    "            for v in range(8):\n",
    "                for u in range(8):\n",
    "                    # undo the zigzag ordering (implicitly, without having to store the unscrambled array)\n",
    "                    S_vu = coeffs[DCT_BASIS_IDX_TO_ZIGZAG[v * 8 + u]]\n",
    "\n",
    "                    # NOTE that we are now looking up precomputed values here!\n",
    "                    s_yx += S_vu * idct_demo_precompute[y * 8 * 8 * 8 + x * 8 * 8 + v * 8 + u]\n",
    "\n",
    "            output[(px_ybase + y) * px_w + (px_xbase + x)] = s_yx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This version of the code is indeed faster as we were hoping! (It is still not exactly _fast_. It turns out that image processing is quite computationally expensive!)\n",
    "\n",
    "In order to see \"those thingies\" featured in the meme at the very beginning of this notebook, we can plot the 4096-element array as an $8 \\times 8$ grid of $8 \\times 8$ values. In order to make it visually match, we just need to shuffle some indices around:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo_show_idct_basis_biglist():\n",
    "    fig, axs = plt.subplots(8, 8)\n",
    "    for v in range(8):\n",
    "        for u in range(8):\n",
    "            shuffled_basis_func = []\n",
    "            for y in range(8):\n",
    "                row = []\n",
    "                for x in range(8):\n",
    "                    row.append(idct_demo_precompute[y * 8 * 8 * 8 + x * 8 * 8 + v * 8 + u])\n",
    "                shuffled_basis_func.append(row)\n",
    "            axs[v, u].pcolormesh(shuffled_basis_func)\n",
    "demo_show_idct_basis_biglist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we have made use of the fact that multiplication is _commutative_, but we haven't actually needed the linearity property yet. If all we wanted to do was to make the code somewhat faster, we can stop here.\n",
    "\n",
    "To help prepare for later linear algebra hax, we want to rearrange the 4096-element array into a $64 \\times 64$ nested array ordered more like the above plots. We want the first index to correspond to one specific subplot and the second index to correspond to one pixel within the subplot. In order to do this, we swap the `xy` and `uv` loop order:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_idct_basis():\n",
    "    result = [None] * 64\n",
    "    # xy and vu loop order have been changed, but it still loops over all the same things\n",
    "    for v in range(8):\n",
    "        for u in range(8):\n",
    "            # intermediate array\n",
    "            basis_func = [None] * 64\n",
    "\n",
    "            # constant\n",
    "            # moved out of xy loop, since the value does not depend on xy\n",
    "            if u == 0 and v == 0:\n",
    "                # 1/4 * 1/sqrt(2) * 1/sqrt(2) = 1/4 * 1/2 = 1/8\n",
    "                a = 1 / 8\n",
    "            elif u == 0 or v == 0:\n",
    "                # 1/4 * 1/sqrt(2) * 1\n",
    "                a = 1 / (4 * sqrt(2))\n",
    "            else:\n",
    "                a = 1 / 4\n",
    "\n",
    "            # now we finally get the xy loop\n",
    "            for y in range(8):\n",
    "                for x in range(8):\n",
    "                    # constant * cosine_1 * cosine_2\n",
    "                    val = a * cos(((2*x+1)*u/16)*pi) * cos(((2*y+1)*v/16)*pi)\n",
    "                    basis_func[y*8 + x] = val\n",
    "\n",
    "            # save it\n",
    "            result[v*8 + u] = basis_func\n",
    "    return result\n",
    "idct_basis = compute_idct_basis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variable `idct_basis` now contains something much closer to \"those thingies,\" and we can once again plot them with a little bit of code (although we are forced to do yet more rearranging so that the plots show up the way we want):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo_show_idct_basis():\n",
    "    fig, axs = plt.subplots(8, 8)\n",
    "    for v in range(8):\n",
    "        for u in range(8):\n",
    "            basis_func = idct_basis[v*8 + u]\n",
    "            shuffled_basis_func = []\n",
    "            for y in range(8):\n",
    "                row = []\n",
    "                for x in range(8):\n",
    "                    row.append(basis_func[y*8 + x])\n",
    "                shuffled_basis_func.append(row)\n",
    "            axs[v, u].pcolormesh(shuffled_basis_func)\n",
    "demo_show_idct_basis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A refactoring step which *does* actually require the linearity property is to swap the `xy` and `uv` loop order in the IDCT computation itself just as we swapped them in the precomputation. This groups together the memory lookups which depend on `uv` and separates them from the ones which depend on `xy`, which doesn't speed up Python but can speed up vectorized native code implementations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the IDCT on an 8x8 data unit, with rearranged precomputation\n",
    "def idct_block(output, mcu_x, mcu_y, mcu_w, coeffs):\n",
    "    px_xbase = mcu_x * 8\n",
    "    px_ybase = mcu_y * 8\n",
    "    px_w = mcu_w * 8\n",
    "\n",
    "    # xy and vu loop order have been changed, but it still loops over all the same things\n",
    "    for v in range(8):\n",
    "        for u in range(8):\n",
    "            # undo the zigzag ordering (implicitly, without having to store the unscrambled array)\n",
    "            S_vu = coeffs[DCT_BASIS_IDX_TO_ZIGZAG[v * 8 + u]]\n",
    "            # look up the appropriate basis function corresponding to this coefficient\n",
    "            this_coeff_basis_func = idct_basis[v * 8 + u]\n",
    "\n",
    "            for y in range(8):\n",
    "                for x in range(8):\n",
    "                    # sum is rearranged a bit\n",
    "                    output[(px_ybase + y) * px_w + (px_xbase + x)] += S_vu * this_coeff_basis_func[y * 8 + x]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This final version `idct_block` function takes advantage of linearity as described. Convince yourself that swapping the `xy` and `uv` loop order results in the exact same values being summed in either case!\n",
    "\n",
    "Now that we have an implementation of the IDCT, we can move on to performing IDCT on all color components of the entire image. This will take a while. Real implementations gain additional speedups by exploiting mathematical symmetries and patterns in the DCT coefficients (using _fast cosine transform_ algorithms), but we will not be doing that in this proof-of-concept. By *not* doing this, we keep the ability to operate with _arbitrary linear transforms_, which we will make use of for demos later in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute all IDCTs for an entire image component\n",
    "# also happens to rearrange from a list of MCUs into the final 2-D grid\n",
    "def idct_component(mcus, sof_sos_info, component_i):\n",
    "    mcu_h = divroundup(sof_sos_info.y, 8)\n",
    "    mcu_w = divroundup(sof_sos_info.x, 8)\n",
    "    # initialize the output to zeros\n",
    "    img = [0] * (mcu_w * 8) * (mcu_h * 8)\n",
    "    for y_mcu in range(mcu_h):\n",
    "        for x_mcu in range(mcu_w):\n",
    "            # process one MCU\n",
    "            mcu_i = y_mcu * mcu_w + x_mcu\n",
    "            coeffs = mcus[mcu_i][component_i]\n",
    "            idct_block(img, x_mcu, y_mcu, mcu_w, coeffs)\n",
    "            if mcu_i and mcu_i % 5000 == 0:\n",
    "                print(f\"IDCT done on MCU {mcu_i}/{len(mcus)}\")\n",
    "    return img\n",
    "\n",
    "# compute all IDCTs for an entire image (all three components)\n",
    "def idct_image(mcus, sof_sos_info):\n",
    "    components = [None] * 3\n",
    "    for component_i in range(3):\n",
    "        print(f\"Working on component {component_i}\")\n",
    "        components[component_i] = idct_component(mcus, sof_sos_info, component_i)\n",
    "    return components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_jpeg_components = idct_image(decode_jpeg_mcus, parsed_sof_sos_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what it gives us as a result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo_plot_components():\n",
    "    for component_i in range(3):\n",
    "        ylist = []\n",
    "        h = divroundup(parsed_sof_sos_info.y, 8) * 8\n",
    "        w = divroundup(parsed_sof_sos_info.x, 8) * 8\n",
    "        for y in range(h):\n",
    "            xlist = []\n",
    "            for x in range(w):\n",
    "                xlist.append(decoded_jpeg_components[component_i][y * w + x])\n",
    "            ylist.append(xlist)\n",
    "        plt.pcolormesh(ylist[::-1])\n",
    "        plt.show()\n",
    "demo_plot_components()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Color space conversion\n",
    "\n",
    "We now have three image components, and we want to convert this into \"real\" colors rather than the false-color representation used by the above plots. This necessitates doing _color space conversion_.\n",
    "\n",
    "JPEG itself does not actually specify *how* colors are encoded into the three image components. This image actually contains an [ICC profile](https://en.wikipedia.org/wiki/ICC_profile) specifying how this should be done, and this ICC profile is stored in the APP2 marker segment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo_show_dump_icc():\n",
    "    for seg in parsed_jpeg.marker_segs:\n",
    "        if seg.marker == 0xffe2:\n",
    "            print(seg.data[:32])\n",
    "            with open(\"test.icc\", 'wb') as f:\n",
    "                f.write(seg.data[len(b'ICC_PROFILE\\x00\\x00\\x00'):])\n",
    "demo_show_dump_icc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we open this ICC profile in some software which understands it (e.g. in the builtin macOS viewer), we find that it is describing [sRGB](https://en.wikipedia.org/wiki/SRGB), an average model of computer monitors in the late 1990s and the default color space for anything which does not have an ICC profile.\n",
    "\n",
    "The colors in the JPEG are encoded using Y'CbCr, where Y' corresponds to brightness (black and white), Cb corresponds to \"blueness\", and Cr corresponds to \"redness\". Y' is also referred to as _luma_ and Cb and Cr as _chroma_. Color management is a very complicated topic which is *very* out of scope for this tutorial (also, I the author don't understand it well), so we will simply blindly copy Y'CbCr <-> sRGB conversion formulas from Wikipedia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_color_space(components, w):\n",
    "    h = len(components[0]) // w\n",
    "    im = Image.new('RGB', (w, h))\n",
    "    im_data = im.load()\n",
    "\n",
    "    for y in range(h):\n",
    "        for x in range(w):\n",
    "            y_ = components[0][y * w + x]\n",
    "            cb = components[1][y * w + x]\n",
    "            cr = components[2][y * w + x]\n",
    "\n",
    "            # adjust bias (A.3.1)\n",
    "            y_ += 128\n",
    "            cb += 128\n",
    "            cr += 128\n",
    "\n",
    "            # convert to RGB\n",
    "            r = 298.082/256 * y_ + 408.582/256 * cr - 222.921\n",
    "            g = 298.082/256 * y_ - 100.291/256 * cb - 208.120/256 * cr + 135.576\n",
    "            b = 298.082/256 * y_ + 516.412/256 * cb - 276.836\n",
    "\n",
    "            # round and clamp\n",
    "            r = clamp(r)\n",
    "            g = clamp(g)\n",
    "            b = clamp(b)\n",
    "\n",
    "            im_data[(x, y)] = (r, g, b)\n",
    "\n",
    "    return im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_decoded_image = convert_color_space(decoded_jpeg_components, divroundup(parsed_sof_sos_info.x, 8) * 8)\n",
    "display(final_decoded_image)\n",
    "final_decoded_image.save(\"our_own_decode.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SUCCESS!!!** We have successfully decoded the image!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding, very suboptimally\n",
    "\n",
    "Now that we have successfully done some decoding, let's try to invert all of the steps and encode our own JPEG. We will start with this (much smaller) image of Tux:\n",
    "\n",
    "![Tux the penguin](imgs/Tux2.png)\n",
    "\n",
    "This image is just under 40000 bytes.\n",
    "\n",
    "For our first attempt, we are going to _intentionally_ make some suboptimal choices. This will help highlight where choices _can_ even be made, and it will also give us something to compare against.\n",
    "\n",
    "### Loading the source image\n",
    "\n",
    "The first thing we are going to need to do is to load the source image and turn it into a grid of pixel values. Since this is not a tutorial about the PNG format, we are going to use an existing software library to do this. In the meantime, we will also perform color space conversion to Y'CbCr as well as padding out the image dimensions to a multiple of 8. (JPEG can only really encode images whose dimensions align with the size of its minimum coded units. Everything else has to be padded with filler data. The recommendation is to pad by repeating the edge-most pixels, but we will simply pad with white (which, in this case, does the same thing).)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_png_and_color_convert(filename):\n",
    "    # load the image, making sure we can get RGB pixel values\n",
    "    im = Image.open(filename)\n",
    "    (real_w, real_h) = im.size\n",
    "    rgb_im = im.convert('RGB')\n",
    "    im_data = rgb_im.load()\n",
    "\n",
    "    # pad the size\n",
    "    pad_w = divroundup(real_w, 8) * 8\n",
    "    pad_h = divroundup(real_h, 8) * 8\n",
    "    print(f\"Padding size ({real_w}, {real_h}) -> ({pad_w}, {pad_h})\")\n",
    "\n",
    "    # output image components\n",
    "    y_img = [0] * (pad_w * pad_h)\n",
    "    cb_img = [0] * (pad_w * pad_h)\n",
    "    cr_img = [0] * (pad_w * pad_h)\n",
    "\n",
    "    for y in range(pad_h):\n",
    "        for x in range(pad_w):\n",
    "            # get pixel, or padding in white\n",
    "            if x < real_w and y < real_h:\n",
    "                (r, g, b) = im_data[x, y]\n",
    "            else:\n",
    "                (r, g, b) = (255, 255, 255)\n",
    "            \n",
    "            # convert\n",
    "            y_ = clamp(16 + 65.481/255*r + 128.553/255*g + 24.966/255*b)\n",
    "            cb = clamp(128 - 37.797/255*r - 74.203/255*g + 112.0/255*b)\n",
    "            cr = clamp(128 + 112.0/255*r - 93.786/255*g - 18.214/255*b)\n",
    "            \n",
    "            # perform level shift (A.3.1)\n",
    "            y_ -= 128\n",
    "            cb -= 128\n",
    "            cr -= 128\n",
    "\n",
    "            y_img[y * pad_w + x] = y_\n",
    "            cb_img[y * pad_w + x] = cb\n",
    "            cr_img[y * pad_w + x] = cr\n",
    "    \n",
    "    return (y_img, cb_img, cr_img, (real_w, real_h), pad_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(to_encode_y, to_encode_cb, to_encode_cr, to_encode_sz, to_encode_w) = load_png_and_color_convert(\"imgs/Tux2.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo_plot_to_encode_components():\n",
    "    # reshuffle to array of arrays\n",
    "    pad_h = len(to_encode_y) // to_encode_w\n",
    "    def conv_one(img):\n",
    "        ylist = []\n",
    "        for y in range(pad_h):\n",
    "            xlist = []\n",
    "            for x in range(to_encode_w):\n",
    "                xlist.append(img[y * to_encode_w + x])\n",
    "            ylist.append(xlist)\n",
    "        return ylist[::-1]\n",
    "    # plot\n",
    "    plt.pcolormesh(conv_one(to_encode_y))\n",
    "    plt.show()\n",
    "    plt.pcolormesh(conv_one(to_encode_cb))\n",
    "    plt.show()\n",
    "    plt.pcolormesh(conv_one(to_encode_cr))\n",
    "    plt.show()\n",
    "demo_plot_to_encode_components()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The solid colors in this image as well as the bright colors of the fursuits in the pride parade _really_ show how the Cb and Cr color components correspond to different shades of colors.\n",
    "\n",
    "### Performing the forward DCT\n",
    "\n",
    "The forward DCT is performed in a very similar way to performing the inverse DCT. The same property of linearity applies here as well, as does the existence of fast algorithms which we will not be using.\n",
    "\n",
    "We start by precomputing the basis functions that will be used in the forwards direction. Note the different ordering of the `for` loops compared to the IDCT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_fdct_basis():\n",
    "    ret = [None] * 64\n",
    "    for y in range(8):\n",
    "        for x in range(8):\n",
    "            basis_func = [None] * 64\n",
    "\n",
    "            for v in range(8):\n",
    "                for u in range(8):\n",
    "                    # constant\n",
    "                    if u == 0 and v == 0:\n",
    "                        a = 1 / 8\n",
    "                    elif u == 0 or v == 0:\n",
    "                        a = 1 / (4 * sqrt(2))\n",
    "                    else:\n",
    "                        a = 1 / 4\n",
    "                \n",
    "                    # cosines\n",
    "                    val = a * cos(((2*x+1)*u/16)*pi) * cos(((2*y+1)*v/16)*pi)\n",
    "                    basis_func[v*8 + u] = val\n",
    "            \n",
    "            # save it\n",
    "            ret[y*8 + x] = basis_func\n",
    "    return ret\n",
    "fdct_basis = compute_fdct_basis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo_show_fdct_basis():\n",
    "    fig, axs = plt.subplots(8, 8)\n",
    "    for y in range(8):\n",
    "        for x in range(8):\n",
    "            basis_func = fdct_basis[y*8 + x]\n",
    "            shuffled_basis_func = []\n",
    "            for v in range(8):\n",
    "                row = []\n",
    "                for u in range(8):\n",
    "                    row.append(basis_func[v*8 + u])\n",
    "                shuffled_basis_func.append(row)\n",
    "            axs[y, x].pcolormesh(shuffled_basis_func)\n",
    "demo_show_fdct_basis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are differently-shaped patterns compared to when we were performing the IDCT, and there's clearly still some kind of \"stripey checkerboard\" structure, but the details are out of scope for this discussion. (Each subplot corresponds to the frequencies that would be found in a source image containing only one nonzero pixel at a particular spot. These frequencies depend on how the finite input data gets turned into a periodic function.)\n",
    "\n",
    "We can now compute the FDCT and rearrange the coefficients into zigzag order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the FDCT on an 8x8 data unit\n",
    "def fdct_block(input, xbase, ybase, input_w):\n",
    "    # output coefficient array\n",
    "    coeffs = [0] * 64\n",
    "    for y in range(8):\n",
    "        for x in range(8):\n",
    "            # get the pixel value\n",
    "            px = input[(ybase + y) * input_w + (xbase + x)]\n",
    "            # select the basis function\n",
    "            basis_func = fdct_basis[y*8 + x]\n",
    "            # accumulate it into the output, relying on the superposition principle\n",
    "            for coeff_i in range(64):\n",
    "                # implicit zigzag reordering happens here\n",
    "                coeffs[DCT_BASIS_IDX_TO_ZIGZAG[coeff_i]] += px * basis_func[coeff_i]\n",
    "    \n",
    "    return coeffs\n",
    "\n",
    "# compute all FDCTs for an entire image component\n",
    "# also chunk them into blocks (which will later get interleaved to form MCUs)\n",
    "def fdct_component(input, input_w):\n",
    "    input_h = len(input) // input_w\n",
    "    mcu_w = divroundup(input_w, 8)\n",
    "    mcu_h = divroundup(input_h, 8)\n",
    "    mcus = []\n",
    "    for y_mcu in range(mcu_h):\n",
    "        for x_mcu in range(mcu_w):\n",
    "            # process one unit\n",
    "            mcus.append(fdct_block(input, x_mcu * 8, y_mcu * 8, input_w))\n",
    "\n",
    "    return mcus\n",
    "\n",
    "# compute all FDCTs for an entire image (all three components)\n",
    "def fdct_image(y_, cb, cr, input_w):\n",
    "    # compute\n",
    "    y_units = fdct_component(y_, input_w)\n",
    "    cb_units = fdct_component(cb, input_w)\n",
    "    cr_units = fdct_component(cr, input_w)\n",
    "\n",
    "    # debug assertion\n",
    "    assert len(y_units) == len(cb_units)\n",
    "    assert len(y_units) == len(cr_units)\n",
    "\n",
    "    # combine into array of three arrays\n",
    "    return list(zip(y_units, cb_units, cr_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_encode_mcus = fdct_image(to_encode_y, to_encode_cb, to_encode_cr, to_encode_w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, since we have a decoder implemented already, we can check our work by running the output through the decoder. We just have to make up a `JpegSof0SosInfo` containing the appropriate dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo_check_work_at_fdct_step():\n",
    "    info = JpegSof0SosInfo(to_encode_w, len(to_encode_y) // to_encode_w, None)\n",
    "    check_idct = idct_image(to_encode_mcus, info)\n",
    "    check_decoded_im = convert_color_space(check_idct, to_encode_w)\n",
    "    display(check_decoded_im)\n",
    "demo_check_work_at_fdct_step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far so good!\n",
    "\n",
    "At this point, it's useful to consider what implementation _choices_ we have already made. Even though the DCT step doesn't involve flexible parameters the way later steps such as quantization do, we _have_ already made several design choices.\n",
    "\n",
    "One design choice we have made is the _precision_ to use for intermediate values. Even though input sample values and the output of the quantization step are all integers, values within the DCT computation step are (conceptually) real numbers. For simplicity, we have used Python's built-in floating point numbers, which are [IEEE 754 binary64](https://en.wikipedia.org/wiki/Double-precision_floating-point_format). Because of the later quantization step, this level of precision is not necessary, and implementations which do not use floating point are possible and can perform better.\n",
    "\n",
    "Another design choice we have made is the padding to a multiple of 8 pixels, as explained before. Even though the extra pixels themselves will get discarded when decoding, they can affect visible pixels as the image quality is decreased (made more lossy).\n",
    "\n",
    "So far, the design choices we have made have been rather minor, affecting things such as performance or only small parts of the image. The next choice we are about to make will be way more significant.\n",
    "\n",
    "### Quantization\n",
    "\n",
    "The next step after performing the DCT is to perform _quantization_. Quantization is simple to _compute_ -- just divide each coefficient by the corresponding entry in a _quantization table_, and then round to the closest integer. Except... how do you pick a quantization table? The standard gives two _examples_ in Table K.1 and Table K.2, but it very explicitly does not specify how else it is possible to construct these.\n",
    "\n",
    "In fact, this is a significant source of variability between different pieces of software, cameras, and other devices which output JPEG! Not only are there several patents in this area, the variability is notable enough to be [useful for forensics](https://dfrws.org/sites/default/files/session-files/2008_USA_pres-using_jpeg_quantization_tables_to_identify_imagery_processed_by_software.pdf)!\n",
    "\n",
    "The values in the quantization tables strongly affect the trade-off between quality and output size. In fact, software which offers a \"quality\" slider typically implements it as different choices or rescaling of the quantization tables.\n",
    "\n",
    "There is also the choice of _how many_ quantization tables to use. The standard allows for up to four tables to be used at once, but implementations can choose to use fewer. Many implementations use one table for luma and the same second table for both of the chroma components (our original image does this). It is also possible to use different tables for each chroma component.\n",
    "\n",
    "For this example, we will build it to support using three separate tables (one for each component), but we will start by hardcoding these tables to all 1s. We will tweak them later to see how they affect the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantize_image(mcus, tables):\n",
    "    assert len(tables) == 3, \"need three quantization tables\"\n",
    "    for i in range(len(mcus)):\n",
    "        for component_i in range(3):\n",
    "            for coeff_i in range(64):\n",
    "                mcus[i][component_i][coeff_i] = round(mcus[i][component_i][coeff_i] / tables[component_i][coeff_i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_quant_tables = [[1] * 64, [1] * 64, [1] * 64]\n",
    "quantize_image(to_encode_mcus, encode_quant_tables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Miscellaneous operations\n",
    "\n",
    "At this point, we need to perform the rest of the \"miscellaneous\" operations described in the overview, namely implementing the DC value predictor (a subtraction) and run-length encoding the zeros in the AC coefficients. Unlike when decoding, the way in which we have structured our code makes it straightforward to do this as a distinct step (we have not yet interleaved all of the resulting bits).\n",
    "\n",
    "We will also prepare the magnitude encoding for coefficients so that we have a list of symbols (to be entropy coded) followed by raw bits (to be appended directly).\n",
    "\n",
    "In this demonstration, we will not support using a restart interval. All MCUs will be coded in one go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns (ssss, bits)\n",
    "def encode_coeff_mag(coeff):\n",
    "    if coeff == 0:\n",
    "        return (0, 0)\n",
    "\n",
    "    for i in range(1, 12):\n",
    "        mask = (1 << i) - 1\n",
    "        if abs(coeff) <= mask:\n",
    "            if coeff > 0:\n",
    "                return (i, coeff & mask)\n",
    "            else:\n",
    "                return (i, (coeff - 1) & mask)\n",
    "    \n",
    "    assert False, \"coefficient out of range\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# every token is a symbol (which will be Huffman coded), followed by a string of _nbits_ bits\n",
    "JpegEncodingToken = namedtuple('JpegEncodingToken', ['sym', 'bits', 'nbits'])\n",
    "\n",
    "def encode_misc_ops(mcus):\n",
    "    # mcus is an array of three arrays of 64 coefficients\n",
    "    # [[mcu0_y, mcu0_cb, mcu0_cr], [mcu1_y, mcu1_cb, mcu1_cr], ...]\n",
    "\n",
    "    # output will be a 3-array of arrays of tuples, where the second element is an array\n",
    "    # (toks_y, toks_cb, toks_cr)\n",
    "    # where each toks_X is\n",
    "    # [(dc0, [ac0_0, ac0_1, ...]), (dc1, [ac1_0, ac1_1, ...]), ...]\n",
    "    # each one will be a JpegEncodingToken\n",
    "    output = [[], [], []]\n",
    "\n",
    "    for mcu_i in range(len(mcus)):\n",
    "        for component_i in range(3):\n",
    "            # get previous DC coefficient\n",
    "            if mcu_i == 0:\n",
    "                dc_pred = 0\n",
    "            else:\n",
    "                dc_pred = mcus[mcu_i - 1][component_i][0]\n",
    "            \n",
    "            # this block's coefficients\n",
    "            coeffs = mcus[mcu_i][component_i]\n",
    "\n",
    "            # encode DC coefficient\n",
    "            dc_diff = coeffs[0] - dc_pred\n",
    "            (dc_ssss, dc_bits) = encode_coeff_mag(dc_diff)\n",
    "            dc_tok = JpegEncodingToken(dc_ssss, dc_bits, dc_ssss)\n",
    "\n",
    "            # RLE AC coefficients\n",
    "            ac_toks = []\n",
    "            ac_i = 1\n",
    "            while ac_i < 64:\n",
    "                # if this is all 0s, signal EOB\n",
    "                is_eob = True\n",
    "                for i in range(ac_i, 64):\n",
    "                    if coeffs[i] != 0:\n",
    "                        is_eob = False\n",
    "                        break\n",
    "                if is_eob:\n",
    "                    ac_toks.append(JpegEncodingToken(0, 0, 0))\n",
    "                    break\n",
    "                \n",
    "                # otherwise count the 0s, up to a max of 15\n",
    "                num_zeros = 0\n",
    "                while coeffs[ac_i] == 0 and num_zeros < 15:\n",
    "                    ac_i += 1\n",
    "                    num_zeros += 1\n",
    "                \n",
    "                # encode the zeros, as well as the next coefficient (which might still be 0)\n",
    "                (ac_ssss, ac_bits) = encode_coeff_mag(coeffs[ac_i])\n",
    "                assert ac_ssss in range(0, 11), \"AC coefficient out of range\"\n",
    "                ac_rs = (num_zeros << 4) | ac_ssss\n",
    "                ac_toks.append(JpegEncodingToken(ac_rs, ac_bits, ac_ssss))\n",
    "                ac_i += 1\n",
    "\n",
    "            output[component_i].append((dc_tok, ac_toks))\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_encode_toks = encode_misc_ops(to_encode_mcus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Faking entropy coding\n",
    "\n",
    "The final step before we can start outputting bits is to perform Huffman compression. However, for expository purposes, we will start by intentionally constructing a very bad code without using the Huffman coding technique at all. This will help show that *any* algorithm which generates codes of the correct form can be used.\n",
    "\n",
    "As a design choice, we will make the luma component use one pair of tables while the chroma components share the other pair of tables. In this case, only two pairs of (DC, AC) tables are allowed in total for a baseline JPEG, so it isn't possible to give each chroma component separate tables (it *is* possible under _extended DCT processes_ though).\n",
    "\n",
    "Specifically, we will encode every possible DC coefficient scale value using 4 bits and every possible AC coefficient RS value using 8 bits. Fixed-length codes like this are always prefix-free. As long as we assign every codeword sequentially, it is compatible with the code table representation that JPEG uses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_bad_huff_tables():\n",
    "    dc_table = []\n",
    "    for sym in range(0, 12):\n",
    "        dc_table.append((sym, 4))\n",
    "\n",
    "    ac_table = []\n",
    "    codeword = 0\n",
    "    for rs in range(256):\n",
    "        ssss = rs & 0xf\n",
    "        if ssss > 10:\n",
    "            ac_table.append(None)\n",
    "        else:\n",
    "            ac_table.append((codeword, 8))\n",
    "            codeword += 1\n",
    "\n",
    "    # we will allow for using separate luma/chroma huffman tables, but we don't actually use that here\n",
    "    dc_tables = [dc_table, dc_table]\n",
    "    ac_tables = [ac_table, ac_table]\n",
    "    return (dc_tables, ac_tables)\n",
    "\n",
    "def make_bad_huff_tables_dht():\n",
    "    # data needed for the DHT segment (codes for a given length)\n",
    "    dc_syms_per_len = []\n",
    "    for _i in range(16):\n",
    "        dc_syms_per_len.append([])\n",
    "    for sym in range(0, 12):\n",
    "        dc_syms_per_len[3].append(sym)\n",
    "\n",
    "    ac_syms_per_len = []\n",
    "    for _i in range(16):\n",
    "        ac_syms_per_len.append([])\n",
    "    for rs in range(256):\n",
    "        ssss = rs & 0xf\n",
    "        if ssss > 10:\n",
    "            continue\n",
    "        ac_syms_per_len[7].append(rs)\n",
    "    \n",
    "    dc_tables = [dc_syms_per_len, dc_syms_per_len]\n",
    "    ac_tables = [ac_syms_per_len, ac_syms_per_len]\n",
    "    return (dc_tables, ac_tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_huff_tables = make_bad_huff_tables()\n",
    "encode_huff_tables_for_dht = make_bad_huff_tables_dht()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code helps pack a bitstream into bytes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helpers for packing bits\n",
    "def add_bit(wip, bit):\n",
    "    (wip_bytes, bits_in_last_byte) = wip\n",
    "\n",
    "    if bits_in_last_byte == 8:\n",
    "        wip_bytes.append(bit << 7)\n",
    "        bits_in_last_byte = 1\n",
    "    else:\n",
    "        wip_bytes[-1] |= bit << (7 - bits_in_last_byte)\n",
    "        bits_in_last_byte += 1\n",
    "    \n",
    "    return (wip_bytes, bits_in_last_byte)\n",
    "\n",
    "def add_bits(wip, bits, nbits):\n",
    "    for biti in range(nbits):\n",
    "        b = 1 if bits & (1 << (nbits - 1 - biti)) else 0\n",
    "        wip = add_bit(wip, b)\n",
    "    return wip\n",
    "\n",
    "def pad_out(wip):\n",
    "    for _i in range(wip[1], 8):\n",
    "        wip = add_bit(wip, 1)\n",
    "    return wip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code performs Huffman coding and byte stuffing, such that the bitstream is a complete number of bytes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_huffman(toks, dc_tables, ac_tables):\n",
    "    wip = ([], 8)\n",
    "\n",
    "    # debugging sanity check\n",
    "    assert len(toks) == 3\n",
    "    assert len(toks[0]) == len(toks[1])\n",
    "    assert len(toks[0]) == len(toks[2])\n",
    "\n",
    "    for tok_i in range(len(toks[0])):\n",
    "        for component_i in range(3):\n",
    "            # select the right tables given the component\n",
    "            if component_i == 0:\n",
    "                dc_table = dc_tables[0]\n",
    "                ac_table = ac_tables[0]\n",
    "            else:\n",
    "                dc_table = dc_tables[1]\n",
    "                ac_table = ac_tables[1]\n",
    "            \n",
    "            # get the tokens to be encoded\n",
    "            (dc, ac) = toks[component_i][tok_i]\n",
    "\n",
    "            # DC first\n",
    "            dc_scale = dc_table[dc.sym]\n",
    "            wip = add_bits(wip, *dc_scale)\n",
    "            wip = add_bits(wip, dc.bits, dc.nbits)\n",
    "\n",
    "            # now AC\n",
    "            for ac_i in ac:\n",
    "                ac_rs = ac_table[ac_i.sym]\n",
    "                wip = add_bits(wip, *ac_rs)\n",
    "                wip = add_bits(wip, ac_i.bits, ac_i.nbits)\n",
    "    \n",
    "    # pad out to a full byte\n",
    "    wip = pad_out(wip)\n",
    "\n",
    "    # perform byte stuffing (F.1.2.3))\n",
    "    ret = b''\n",
    "    for b in wip[0]:\n",
    "        ret += bytes([b])\n",
    "        if b == 0xff:\n",
    "            ret += b'\\x00'\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can finally encode the main bitstream containing most of the image data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_encode_bitstream = encode_huffman(to_encode_toks, *encode_huff_tables)\n",
    "print(f\"Bitstream encoded to {len(to_encode_bitstream)} bytes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we just have to write various file headers and we will have a complete JPEG!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pack_final_jpeg(bitstream, w, h, quant_tables, huff_tables):\n",
    "    ret = b''\n",
    "\n",
    "    # SOI\n",
    "    ret += b'\\xff\\xd8'\n",
    "\n",
    "    # DQT\n",
    "    assert len(quant_tables) == 3, \"expected 3 quantization tables\"\n",
    "    ret += struct.pack(\">HH\", 0xffdb, 65 * 3 + 2)\n",
    "    for i in range(3):\n",
    "        ret += bytes([i])\n",
    "        ret += bytes(quant_tables[i])\n",
    "\n",
    "    # DHT\n",
    "    dht = b''\n",
    "    for Tc in range(2):\n",
    "        for Th in range(2):\n",
    "            dht += bytes([(Tc << 4) | Th])\n",
    "            t = huff_tables[Tc][Th]\n",
    "            for len_ in range(16):\n",
    "                dht += bytes([len(t[len_])])\n",
    "            for len_ in range(16):\n",
    "                dht += bytes(t[len_])\n",
    "    ret += struct.pack(\">HH\", 0xffc4, len(dht) + 2)\n",
    "    ret += dht\n",
    "\n",
    "    # SOF\n",
    "    ret += struct.pack(\">HHBHHBBBBBBBBBB\", 0xffc0, 17, 8, h, w, 3, 1, 0x11, 0, 2, 0x11, 1, 3, 0x11, 2)\n",
    "\n",
    "    # SOS\n",
    "    ret += struct.pack(\">HHBBBBBBBBBB\", 0xffda, 12, 3, 1, 0x00, 2, 0x11, 3, 0x11, 0, 63, 0)\n",
    "\n",
    "    ret += bitstream\n",
    "    \n",
    "    # EOI\n",
    "    ret += b'\\xff\\xd9'\n",
    "    \n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "our_encoded_jpeg = pack_final_jpeg(to_encode_bitstream, *to_encode_sz, encode_quant_tables, encode_huff_tables_for_dht)\n",
    "print(f\"Final compressed output is {len(our_encoded_jpeg)} bytes\")\n",
    "with open(\"our_own_encode.jpg\", 'wb') as f:\n",
    "    f.write(our_encoded_jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, we've made a JPEG file! It's... larger than the original PNG.\n",
    "\n",
    "![our result](our_own_encode.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entropy coding a bit more seriously\n",
    "\n",
    "One immediate improvement we can make is to improve the Huffman coding step. Annex K gives both an _example_ of an algorithm which can be used to compute a Huffman code as well as an example precomputed code. Let's see how well the precomputed code does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# given a list of values at each length, assign them codewords systematically\n",
    "def reassign_huff_codes(syms_per_len, print_dbg=False):\n",
    "    table = [None] * 256\n",
    "    min_code_len = 0\n",
    "    for (i, syms) in enumerate(syms_per_len):\n",
    "        if len(syms) != 0:\n",
    "            min_code_len = i + 1\n",
    "            break\n",
    "    assert min_code_len > 0, \"no codewords!\"\n",
    "\n",
    "    code_wip = 0\n",
    "    for code_len in range(min_code_len, 17):\n",
    "        code_vals_of_len = syms_per_len[code_len - 1]\n",
    "        for code_val in code_vals_of_len:\n",
    "            if print_dbg:\n",
    "                print(f\"{code_wip:0{code_len}b} = 0x{code_val:02x}\")\n",
    "            # skip 256 (used for reserving the all-1s code)\n",
    "            if code_val != 256:\n",
    "                table[code_val] = (code_wip, code_len)\n",
    "            code_wip += 1\n",
    "        code_wip <<= 1\n",
    "    \n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_annex_k_example_huff_tables():\n",
    "    dc_luma_syms_per_len = [\n",
    "        [],                 # 1\n",
    "        [0],                # 2\n",
    "        [1, 2, 3, 4, 5],    # 3\n",
    "        [6],                # 4\n",
    "        [7],                # 5\n",
    "        [8],                # 6\n",
    "        [9],                # 7\n",
    "        [10],               # 8\n",
    "        [11],               # 9\n",
    "        [], [], [], [], [], [], [],\n",
    "    ]\n",
    "    dc_chroma_syms_per_len = [\n",
    "        [],                 # 1\n",
    "        [0, 1, 2],          # 2\n",
    "        [3],                # 3\n",
    "        [4],                # 4\n",
    "        [5],                # 5\n",
    "        [6],                # 6\n",
    "        [7],                # 7\n",
    "        [8],                # 8\n",
    "        [9],                # 9\n",
    "        [10],               # 10\n",
    "        [11],               # 11\n",
    "        [], [], [], [], [],\n",
    "    ]\n",
    "    ac_luma_syms_per_len = [\n",
    "        [],\n",
    "        [0x01, 0x02],\n",
    "        [0x03],\n",
    "        [0x00, 0x04, 0x11],\n",
    "        [0x05, 0x12, 0x21],\n",
    "        [0x31, 0x41],\n",
    "        [0x06, 0x13, 0x51, 0x61],\n",
    "        [0x07, 0x22, 0x71],\n",
    "        [0x14, 0x32, 0x81, 0x91, 0xA1],\n",
    "        [0x08, 0x23, 0x42, 0xB1, 0xC1],\n",
    "        [0x15, 0x52, 0xD1, 0xF0],\n",
    "        [0x24, 0x33, 0x62, 0x72],\n",
    "        [],\n",
    "        [],\n",
    "        [0x82],\n",
    "        [\n",
    "            0x09, 0x0A, 0x16, 0x17, 0x18, 0x19, 0x1A, 0x25, 0x26, 0x27, 0x28, 0x29, 0x2A, 0x34, 0x35, 0x36,\n",
    "            0x37, 0x38, 0x39, 0x3A, 0x43, 0x44, 0x45, 0x46, 0x47, 0x48, 0x49, 0x4A, 0x53, 0x54, 0x55, 0x56,\n",
    "            0x57, 0x58, 0x59, 0x5A, 0x63, 0x64, 0x65, 0x66, 0x67, 0x68, 0x69, 0x6A, 0x73, 0x74, 0x75, 0x76,\n",
    "            0x77, 0x78, 0x79, 0x7A, 0x83, 0x84, 0x85, 0x86, 0x87, 0x88, 0x89, 0x8A, 0x92, 0x93, 0x94, 0x95,\n",
    "            0x96, 0x97, 0x98, 0x99, 0x9A, 0xA2, 0xA3, 0xA4, 0xA5, 0xA6, 0xA7, 0xA8, 0xA9, 0xAA, 0xB2, 0xB3,\n",
    "            0xB4, 0xB5, 0xB6, 0xB7, 0xB8, 0xB9, 0xBA, 0xC2, 0xC3, 0xC4, 0xC5, 0xC6, 0xC7, 0xC8, 0xC9, 0xCA,\n",
    "            0xD2, 0xD3, 0xD4, 0xD5, 0xD6, 0xD7, 0xD8, 0xD9, 0xDA, 0xE1, 0xE2, 0xE3, 0xE4, 0xE5, 0xE6, 0xE7,\n",
    "            0xE8, 0xE9, 0xEA, 0xF1, 0xF2, 0xF3, 0xF4, 0xF5, 0xF6, 0xF7, 0xF8, 0xF9, 0xFA\n",
    "        ]\n",
    "    ]\n",
    "    ac_chroma_syms_per_len = [\n",
    "        [],\n",
    "        [0x00, 0x01],\n",
    "        [0x02],\n",
    "        [0x03, 0x11],\n",
    "        [0x04, 0x05, 0x21, 0x31],\n",
    "        [0x06, 0x12, 0x41, 0x51],\n",
    "        [0x07, 0x61, 0x71],\n",
    "        [0x13, 0x22, 0x32, 0x81],\n",
    "        [0x08, 0x14, 0x42, 0x91, 0xA1, 0xB1, 0xC1],\n",
    "        [0x09, 0x23, 0x33, 0x52, 0xF0],\n",
    "        [0x15, 0x62, 0x72, 0xD1],\n",
    "        [0x0A, 0x16, 0x24, 0x34],\n",
    "        [],\n",
    "        [0xE1],\n",
    "        [0x25, 0xF1],\n",
    "        [\n",
    "            0x17, 0x18, 0x19, 0x1A, 0x26, 0x27, 0x28, 0x29, 0x2A, 0x35, 0x36, 0x37, 0x38, 0x39, 0x3A, 0x43,\n",
    "            0x44, 0x45, 0x46, 0x47, 0x48, 0x49, 0x4A, 0x53, 0x54, 0x55, 0x56, 0x57, 0x58, 0x59, 0x5A, 0x63,\n",
    "            0x64, 0x65, 0x66, 0x67, 0x68, 0x69, 0x6A, 0x73, 0x74, 0x75, 0x76, 0x77, 0x78, 0x79, 0x7A, 0x82,\n",
    "            0x83, 0x84, 0x85, 0x86, 0x87, 0x88, 0x89, 0x8A, 0x92, 0x93, 0x94, 0x95, 0x96, 0x97, 0x98, 0x99,\n",
    "            0x9A, 0xA2, 0xA3, 0xA4, 0xA5, 0xA6, 0xA7, 0xA8, 0xA9, 0xAA, 0xB2, 0xB3, 0xB4, 0xB5, 0xB6, 0xB7,\n",
    "            0xB8, 0xB9, 0xBA, 0xC2, 0xC3, 0xC4, 0xC5, 0xC6, 0xC7, 0xC8, 0xC9, 0xCA, 0xD2, 0xD3, 0xD4, 0xD5,\n",
    "            0xD6, 0xD7, 0xD8, 0xD9, 0xDA, 0xE2, 0xE3, 0xE4, 0xE5, 0xE6, 0xE7, 0xE8, 0xE9, 0xEA, 0xF2, 0xF3,\n",
    "            0xF4, 0xF5, 0xF6, 0xF7, 0xF8, 0xF9, 0xFA\n",
    "        ]\n",
    "    ]\n",
    "\n",
    "    dc_luma_lut = reassign_huff_codes(dc_luma_syms_per_len)\n",
    "    dc_chroma_lut = reassign_huff_codes(dc_chroma_syms_per_len)\n",
    "    ac_luma_lut = reassign_huff_codes(ac_luma_syms_per_len)\n",
    "    ac_chroma_lut = reassign_huff_codes(ac_chroma_syms_per_len)\n",
    "\n",
    "    return (\n",
    "        [\n",
    "            [dc_luma_lut, dc_chroma_lut],\n",
    "            [ac_luma_lut, ac_chroma_lut],\n",
    "        ], [\n",
    "            [dc_luma_syms_per_len, dc_chroma_syms_per_len],\n",
    "            [ac_luma_syms_per_len, ac_chroma_syms_per_len],\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(encode_huff_tables, encode_huff_tables_for_dht) = make_annex_k_example_huff_tables()\n",
    "to_encode_bitstream = encode_huffman(to_encode_toks, *encode_huff_tables)\n",
    "print(f\"Bitstream encoded to {len(to_encode_bitstream)} bytes\")\n",
    "our_encoded_jpeg = pack_final_jpeg(to_encode_bitstream, *to_encode_sz, encode_quant_tables, encode_huff_tables_for_dht)\n",
    "print(f\"Final compressed output is {len(our_encoded_jpeg)} bytes\")\n",
    "with open(\"our_own_encode_better_huff.jpg\", 'wb') as f:\n",
    "    f.write(our_encoded_jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow, a reduction of almost 36% compared to the initial attempt, and very slightly smaller than the original PNG too. This intuitively does make some sense: we have compressed the file with a quality of \"100%\" (which is slightly lossy due to rounding to the nearest integer but is otherwise maximum quality) and then applied some \"vaguely similar if you squint at it from afar\" lossless compression (RLE+Huffman in the case of JPEG and LZ77+Huffman in the case of PNG)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Huffman coding, for real this time\n",
    "\n",
    "At this point we are going to *finally* stop avoiding understanding the Huffman coding technique, in order to eke out a bit more compression.\n",
    "\n",
    "As described in the explanation for decoding, we wish to construct a _prefix code_ which is, in some sense, \"optimal.\" Huffman coding happens to be an algorithm which is optimal under the following assumptions:\n",
    "\n",
    "* symbol-by-symbol coding\n",
    "* where each symbol is _independently and identically distributed_\n",
    "* and where the probability distribution is known\n",
    "\n",
    "Let's think about _why_ these assumptions are important. Let's suppose we are trying to compress the data `abababababababababababababababababababababababababababababababababababababababababababababababababababababababababababababababababababababababababababababababababababababababababababababababababababab` (composed of symbols `a` and `b`). The probability of each symbol is exactly 0.5, but the symbol probabilities are clearly not independent (the pattern is always `ab`, there is never `aa` nor `bb`). One optimal symbol-by-symbol prefix code for encoding this information is\n",
    "\n",
    "| Symbol | Code |\n",
    "| ------ | ---- |\n",
    "| `a`    | 0    |\n",
    "| `b`    | 1    |\n",
    "\n",
    "If we use this code, the output will consist of 200 bits. But surely there are better ways to express this pattern? Something like \"repeat 100 copies of `ab`\" perhaps? It seems as if that should only require a handful of bytes? Something like \"this is a repeat\" (and not something else), \"repeat 100 times\", \"repeat the following 2 bytes\", `ab`. This feels like it should only use somewhere around 5 bytes or 40 bits, if each of those pieces of information takes up one byte? If you follow this chain of thinking, then _congratulations_, you have stumbled into _dictionary coders_ such as the LZ (Lempel-Ziv) family of algorithms!\n",
    "\n",
    "One of the most popular _lossless_ data compression algorithms today is _DEFLATE_, which is built around the idea of using LZ77 and Huffman coding together. This is the compression used in ZIP files as well as PNG. However, we cannot use that idea here for JPEG (the spec simply doesn't allow it).\n",
    "\n",
    "Returning to the code we are constructing here, the key idea that David A. Huffman came up with was that the _expected value_ of the lengths of the codewords can be minimized (becoming optimal under our assumptions) by constructing a particular _binary tree_ from the bottom up. However, we cannot directly use Huffman's algorithm here, because:\n",
    "\n",
    "* JPEG codewords have a maximum length\n",
    "* Codewords consisting of all-1s cannot be used\n",
    "\n",
    "The last issue is easy to deal with, and Annex K of the specification suggests a trick: add a dummy symbol (which will never be used) with the lowest possible probability. This dummy symbol will always end up with one of the longest codewords. If codewords are assigned in _canonical_ order, we can easily make sure that, *if* an all-1s codeword were to get used, it would be assigned to this dummy symbol. We can then just delete the dummy symbol and not use it, and then the all-1s codeword will never be used.\n",
    "\n",
    "The first issue is harder to deal with. Annex K suggests an algorithm which adjusts the output of the normal Huffman coding algorithm. It makes modifications to the code to replace longer codewords with shorter ones. However, it is not clear if this algorithm continues to produce optimal codes or _why_ it would continue to do so if it does.\n",
    "\n",
    "We will not be doing that. We will instead use a generalization called the [package-merge algorithm](https://en.wikipedia.org/wiki/Package-merge_algorithm), which is more complex but guaranteed to be optimal (under our assumptions). I don't know why the standard does not suggest this algorithm, but the package-merge algorithm was first published in 1990, only two years before the JPEG standard was released. It's possible that the algorithm wasn't around long enough to make it into the standard. If anybody knows for sure, feel free to chime in!\n",
    "\n",
    "The following is a direct translation of the Wikipedia explanation of the package-merge algorithm into code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_one_optimal_huff_code(sym_freqs):\n",
    "    # each coin/package will be stored as a tuple (total value/weight, [symbol, symbol, ...])\n",
    "\n",
    "    # prepare a \"master\" list of \"coins\" for each symbol, each with value/weight corresponding to its frequency\n",
    "    # this will be reused for each \"denomination\"/width (for a total of L coins for each symbol)\n",
    "    master_coin_list = []\n",
    "    num_syms = 0\n",
    "    for sym in range(len(sym_freqs)):\n",
    "        freq = sym_freqs[sym]\n",
    "        if freq:\n",
    "            num_syms += 1\n",
    "            master_coin_list.append((freq, [sym]))\n",
    "    assert num_syms, \"no syms!\"\n",
    "    # sort by total value\n",
    "    master_coin_list.sort(key=lambda x: x[0])\n",
    "\n",
    "    # this will be the output from the previous iteration to merge in\n",
    "    packaged = []\n",
    "\n",
    "    # go through \"denominations\"/widths from smallest up to largest\n",
    "    for denom_i in reversed(range(1, 17)):\n",
    "        # merge in\n",
    "        package_merge_iter_input = sorted(packaged + master_coin_list, key=lambda x: x[0])\n",
    "\n",
    "        # package in pairs\n",
    "        packaged = []\n",
    "        for i in range(len(package_merge_iter_input) // 2):\n",
    "            c0 = package_merge_iter_input[i * 2]\n",
    "            c1 = package_merge_iter_input[i * 2 + 1]\n",
    "            cout = (c0[0] + c1[0], c0[1] + c1[1])\n",
    "            packaged.append(cout)\n",
    "    \n",
    "    # at this point, the \"denomination\" is 1\n",
    "    # grab the top n-1\n",
    "    selected_result_coins = packaged[:num_syms - 1]\n",
    "\n",
    "    # count how many \"coins\" were used for each symbol\n",
    "    sym_coins = [0] * len(sym_freqs)\n",
    "    for (_weight, syms) in selected_result_coins:\n",
    "        for sym in syms:\n",
    "            sym_coins[sym] += 1\n",
    "\n",
    "    # shuffle into a list of symbols per bit length\n",
    "    syms_per_len = []\n",
    "    for _i in range(16):\n",
    "        syms_per_len.append([])\n",
    "    # skip the dummy symbol\n",
    "    for sym in range(len(sym_freqs) - 1):\n",
    "        coins = sym_coins[sym]\n",
    "        # sanity checks for debugging\n",
    "        if coins == 0:\n",
    "            assert sym_freqs[sym] == 0\n",
    "        if sym_freqs[sym] == 0:\n",
    "            assert coins == 0\n",
    "        else:\n",
    "            # sym is actually used, and it is encoded with a bit string of length `coins`\n",
    "            syms_per_len[coins - 1].append(sym)\n",
    "    \n",
    "    return syms_per_len\n",
    "\n",
    "def compute_optimal_huffman_codes(toks):\n",
    "    # initialize empty frequency counts\n",
    "    dc_freqs = [[0] * 13, [0] * 13]\n",
    "    ac_freqs = [[0] * 257, [0] * 257]\n",
    "    # add a count for the dummy symbol needed to avoid all-1s\n",
    "    dc_freqs[0][-1] = 1\n",
    "    dc_freqs[1][-1] = 1\n",
    "    ac_freqs[0][-1] = 1\n",
    "    ac_freqs[1][-1] = 1\n",
    "\n",
    "    # go through the tokens, get the actual frequencies\n",
    "    for tok_i in range(len(toks[0])):\n",
    "        for component_i in range(3):\n",
    "            # select the right tables given the component\n",
    "            if component_i == 0:\n",
    "                luma_chroma = 0\n",
    "            else:\n",
    "                luma_chroma = 1\n",
    "            \n",
    "            # get the tokens to be encoded\n",
    "            (dc, ac) = toks[component_i][tok_i]\n",
    "\n",
    "            # DC first\n",
    "            assert dc.sym in range(0, 12)\n",
    "            dc_freqs[luma_chroma][dc.sym] += 1\n",
    "\n",
    "            # now AC\n",
    "            for ac_i in ac:\n",
    "                assert ac_i.sym in range(0, 256)\n",
    "                ac_freqs[luma_chroma][ac_i.sym] += 1\n",
    "\n",
    "    # compute the huffman code lengths\n",
    "    dc_luma_syms_per_len = compute_one_optimal_huff_code(dc_freqs[0])\n",
    "    dc_chroma_syms_per_len = compute_one_optimal_huff_code(dc_freqs[1])\n",
    "    ac_luma_syms_per_len = compute_one_optimal_huff_code(ac_freqs[0])\n",
    "    ac_chroma_syms_per_len = compute_one_optimal_huff_code(ac_freqs[1])\n",
    "\n",
    "    # compute the canonical code values\n",
    "    dc_luma_lut = reassign_huff_codes(dc_luma_syms_per_len, True)\n",
    "    dc_chroma_lut = reassign_huff_codes(dc_chroma_syms_per_len, True)\n",
    "    ac_luma_lut = reassign_huff_codes(ac_luma_syms_per_len, True)\n",
    "    ac_chroma_lut = reassign_huff_codes(ac_chroma_syms_per_len, True)\n",
    "\n",
    "    return (\n",
    "        [\n",
    "            [dc_luma_lut, dc_chroma_lut],\n",
    "            [ac_luma_lut, ac_chroma_lut],\n",
    "        ], [\n",
    "            [dc_luma_syms_per_len, dc_chroma_syms_per_len],\n",
    "            [ac_luma_syms_per_len, ac_chroma_syms_per_len],\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(encode_huff_tables, encode_huff_tables_for_dht) = compute_optimal_huffman_codes(to_encode_toks)\n",
    "to_encode_bitstream = encode_huffman(to_encode_toks, *encode_huff_tables)\n",
    "print(f\"Bitstream encoded to {len(to_encode_bitstream)} bytes\")\n",
    "our_encoded_jpeg = pack_final_jpeg(to_encode_bitstream, *to_encode_sz, encode_quant_tables, encode_huff_tables_for_dht)\n",
    "print(f\"Final compressed output is {len(our_encoded_jpeg)} bytes\")\n",
    "with open(\"our_own_encode_better_huff_2.jpg\", 'wb') as f:\n",
    "    f.write(our_encoded_jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow, better tables saves us over 2 KiB! This is a reduction of almost 40% against the initial \"not compressing at all\" attempt, and a 6% improvement over the example tables given in Annex K. This really goes to show that Huffman codes are built around _assumptions_ about probability distributions. Because our example image doesn't match the Annex K example's assumed distribution exactly, we were able to gain some noticeable improvement by building our own code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trading off size and quality\n",
    "\n",
    "All of the tweaking we have been doing so far has been on the _lossless_ side of the JPEG compression. We have not actually done any work on the _lossy_ side of the compression algorithm, and we are still using the all-1s quantization table.\n",
    "\n",
    "Let's blindly copy the examples from Annex K (even though they are not a perfect match for our image, as we haven't used 2:1 horizontal subsampling like they assume) just to see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "ZIGZAG_TO_IDCT_BASIS_IDX = [\n",
    "    0,\n",
    "    1, 8,\n",
    "    16, 9, 2,\n",
    "    3, 10, 17, 24,\n",
    "    32, 25, 18, 11, 4,\n",
    "    5, 12, 19, 26, 33, 40,\n",
    "    48, 41, 34, 27, 20, 13, 6,\n",
    "    7, 14, 21, 28, 35, 42, 49, 56,\n",
    "    57, 50, 43, 36, 29, 22, 15,\n",
    "    23, 30, 37, 44, 51, 58,\n",
    "    59, 52, 45, 38, 31,\n",
    "    39, 46, 53, 60,\n",
    "    61, 54, 47,\n",
    "    55, 62,\n",
    "    63,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_annex_k_example_quant_tables():\n",
    "    luma = [\n",
    "        16, 11, 10, 16, 24, 40, 51, 61,\n",
    "        12, 12, 14, 19, 26, 58, 60, 55,\n",
    "        14, 13, 16, 24, 40, 57, 69, 56,\n",
    "        14, 17, 22, 29, 51, 87, 80, 62,\n",
    "        18, 22, 37, 56, 68, 109, 103, 77,\n",
    "        24, 35, 55, 64, 81, 104, 113, 92,\n",
    "        49, 64, 78, 87, 103, 121, 120, 101,\n",
    "        72, 92, 95, 98, 112, 100, 103, 99,\n",
    "    ]\n",
    "    chroma = [\n",
    "        17, 18, 24, 47, 99, 99, 99, 99,\n",
    "        18, 21, 26, 66, 99, 99, 99, 99,\n",
    "        24, 26, 56, 99, 99, 99, 99, 99,\n",
    "        47, 66, 99, 99, 99, 99, 99, 99,\n",
    "        99, 99, 99, 99, 99, 99, 99, 99,\n",
    "        99, 99, 99, 99, 99, 99, 99, 99,\n",
    "        99, 99, 99, 99, 99, 99, 99, 99,\n",
    "        99, 99, 99, 99, 99, 99, 99, 99,\n",
    "    ]\n",
    "    luma_zz = [luma[ZIGZAG_TO_IDCT_BASIS_IDX[i]] for i in range(64)]\n",
    "    chroma_zz = [chroma[ZIGZAG_TO_IDCT_BASIS_IDX[i]] for i in range(64)]\n",
    "    return [luma_zz, chroma_zz, chroma_zz]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_quant_tables = make_annex_k_example_quant_tables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_encode_mcus = fdct_image(to_encode_y, to_encode_cb, to_encode_cr, to_encode_w)\n",
    "quantize_image(to_encode_mcus, encode_quant_tables)\n",
    "to_encode_toks = encode_misc_ops(to_encode_mcus)\n",
    "(encode_huff_tables, encode_huff_tables_for_dht) = compute_optimal_huffman_codes(to_encode_toks)\n",
    "to_encode_bitstream = encode_huffman(to_encode_toks, *encode_huff_tables)\n",
    "print(f\"Bitstream encoded to {len(to_encode_bitstream)} bytes\")\n",
    "our_encoded_jpeg = pack_final_jpeg(to_encode_bitstream, *to_encode_sz, encode_quant_tables, encode_huff_tables_for_dht)\n",
    "print(f\"Final compressed output is {len(our_encoded_jpeg)} bytes\")\n",
    "with open(\"our_own_encode_annex_k_quant.jpg\", 'wb') as f:\n",
    "    f.write(our_encoded_jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow, this is an >80% reduction in size against our previous best! However, as expected from _lossy_ compression, zooming in on the result will show _compression artifacts_ on the outline of Tux.\n",
    "\n",
    "![annex k quantization](our_own_encode_annex_k_quant.jpg)\n",
    "\n",
    "So, what _exactly_ have we done here?\n",
    "\n",
    "Remember that the quantization operation is a division followed by rounding to the nearest integer. For example, entry 00 of the luma quantization table is 16. This means that all values between (-8, 8) turn into 0 (because 8/16 = 0.5 which rounds to 0, ignoring tie-breaking rules). Likewise, all values between (-24, -8) and (8, 24) turn into $\\mp 1$, values between (-40, -24) and (24, 40) turn into $\\mp 2$, and so on.\n",
    "\n",
    "As values in the quantization table get larger, the range of values which shrink down into each integer gets wider, and the amount of information which gets \"thrown away\" increases. We can try pushing this to an extreme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# horrible, horrible quality\n",
    "encode_quant_tables = [[150] * 64] * 3\n",
    "\n",
    "to_encode_mcus = fdct_image(to_encode_y, to_encode_cb, to_encode_cr, to_encode_w)\n",
    "quantize_image(to_encode_mcus, encode_quant_tables)\n",
    "to_encode_toks = encode_misc_ops(to_encode_mcus)\n",
    "(encode_huff_tables, encode_huff_tables_for_dht) = compute_optimal_huffman_codes(to_encode_toks)\n",
    "to_encode_bitstream = encode_huffman(to_encode_toks, *encode_huff_tables)\n",
    "print(f\"Bitstream encoded to {len(to_encode_bitstream)} bytes\")\n",
    "our_encoded_jpeg = pack_final_jpeg(to_encode_bitstream, *to_encode_sz, encode_quant_tables, encode_huff_tables_for_dht)\n",
    "print(f\"Final compressed output is {len(our_encoded_jpeg)} bytes\")\n",
    "with open(\"our_own_encode_horrible_quant.jpg\", 'wb') as f:\n",
    "    f.write(our_encoded_jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've gained an *additional* >60% size improvement! We've reduced the original PNG image by almost 94%! The file is now only about 2 KiB!\n",
    "\n",
    "But, as a tradeoff:\n",
    "\n",
    "![low quality quantization](our_own_encode_horrible_quant.jpg)\n",
    "\n",
    "_\\*mmm\\*_ crunchy!\n",
    "\n",
    "### Controllable quality\n",
    "\n",
    "As explained previously, _how_ to derive \"good\" quantization tables is complicated and often subjective. Software derived from or inspired by the [Independent JPEG Group](https://ijg.org/)'s code often derives tables by rescaling the Annex K example tables depending on the desired quality as can be seen [here](https://github.com/libjpeg-turbo/libjpeg-turbo/blob/e0e18dea5433e600ea92d60814f13efa40a0d7dd/src/jcparam.c#L132) in libjpeg-turbo. Implementing this will be left as an exercise for the reader."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Altering the DCT, using linear algebra\n",
    "\n",
    "Now that we have a working (even if computationally-focused) implementation, we will explore what the tools found in _linear algebra_ can help us understand about the DCT.\n",
    "\n",
    "If we have a linear function where the inputs and outputs are both finite-dimensional, we can choose to represent the function using a _matrix_. A matrix is simply a two-dimensional array of numbers along with specific procedures for performing calculations (just like how we have procedures for doing arithmetic with everyday base-10 Arabic numerals). Notably, there is a procedure for adding two matrices of the same size (add each corresponding element) and for multiplying two matrices with appropriately-matching sizes (adding up products of the appropriate elements; this procedure can be found in any introductory linear algebra reference).\n",
    "\n",
    "The DCT (both forward and inverse) takes in a set of 64 numbers (which is finite), outputs 64 numbers (again, finite), and is linear. We can therefore represent the forward and inverse transforms as 64 x 64 matrices. Showing this symbolically is once again very procedural, definition-heavy, and not particularly enlightening. It mostly consists of yet more rearranging of calculations. However, with the rearranging we did earlier in our code, the way in which we compute `idct_basis` and `fdct_basis` already *is* a matrix representation of the transforms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo_dct_as_matrix():\n",
    "    print(f\"idct_basis is a list of {len(idct_basis)} items\")\n",
    "    print(f\"each item of idct_basis is a list of {len(idct_basis[0])} numbers\")\n",
    "    print()\n",
    "\n",
    "    idct_basis_np = np.array(idct_basis)\n",
    "    print(f\"idct_basis as a NumPy array is: {idct_basis_np}\")\n",
    "    print(f\"idct_basis dimensions are: {idct_basis_np.shape}\")\n",
    "    print()\n",
    "\n",
    "    fdct_basis_np = np.array(fdct_basis)\n",
    "    print(f\"fdct_basis as a NumPy array is: {fdct_basis_np}\")\n",
    "    print(f\"fdct_basis dimensions are: {fdct_basis_np.shape}\")\n",
    "    print()\n",
    "demo_dct_as_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the point of doing this? A common tool used in mathematics is to start by giving names to particular objects. Once that is done, we often hope to find useful properties that apply more generally across all objects of a similar type (rather than just the specific object in front of us). These properties might not be so easily found without having \"taken a step back\" and looking from a different perspective. This is very similar to the power of using _abstractions_ when programming.\n",
    "\n",
    "One example of this type of more-general property is that a matrix has an _inverse_ if and only if the value of its _determinant_ is nonzero. The determinant is a function which turns a _square_ matrix (one where the two dimensions are the same) into one single value (a _scalar_) according to a particular formula.\n",
    "\n",
    "We can check the determinant of one of our DCT matrices by asking NumPy to compute it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo_idct_det():\n",
    "    idct_np_array = np.array(idct_basis)\n",
    "    print(f\"The determinant of the IDCT as a matrix is: {np.linalg.det(idct_np_array)}\")\n",
    "demo_idct_det()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the determinant is nonzero, we know that the IDCT has an inverse, i.e. a function which we can apply before or after the IDCT in order to get the input we started with. We were of course already assuming this, and we also \"know\" that the inverse of the IDCT is the FDCT, but it is good to see that the theory checks out.\n",
    "\n",
    "In order to be _even more_ sure, we can try to actually combine together the FDCT and the IDCT to see what we get. In order to do this, we need a few more general properties about matrix arithmetic.\n",
    "\n",
    "### Some properties of matrix arithmetic\n",
    "\n",
    "When we apply two matrices to a given input, one way we can compute it is as follows:\n",
    "\n",
    "Suppose we want to find $ABx$, where $A$ and $B$ are matrices and $x$ is the input as a _vector_ (which in this case can be thought of as just a $n$ x $1$ matrix). We can apply $B$ first, get an intermediate result, and then apply $A$ to that result:\n",
    "\n",
    "```text\n",
    "⠀      _____  Bx  _____\n",
    "x ---> | B | ---> | A | ---> ABx\n",
    "       ‾‾‾‾‾      ‾‾‾‾‾\n",
    "\n",
    "```\n",
    "\n",
    "We can also do the computation by first computing $AB$ (which will itself be a matrix) and then applying that to the input:\n",
    "\n",
    "```text\n",
    "⠀    _____\n",
    "A -> | B | -> AB\n",
    "     ‾‾‾‾‾\n",
    "     ______\n",
    "x -> | AB | -> ABx\n",
    "     ‾‾‾‾‾‾\n",
    "```\n",
    "\n",
    "What we *cannot* do (in general, but it does work in special situations) is to apply $A$ to $x$ first and then apply $B$:\n",
    "\n",
    "```text\n",
    "⠀      _____  Ax  _____\n",
    "x ---> | A | ---> | B | ---> BAx != ABx\n",
    "       ‾‾‾‾‾      ‾‾‾‾‾\n",
    "\n",
    "```\n",
    "\n",
    "In mathematical terms, matrix multiplication is _associative_ but not _commutative_.\n",
    "\n",
    "When matrices $A$ and $B$ are _inverses_ of each other, $AB = BA = I$ where $I$ is the _identity matrix_. The identity matrix is a matrix that happens to have the value 1 on the diagonal from the upper-left to the lower-right and 0 everywhere else. In this situation, $A$ and $B$ happen to commute with each other (but *only* with each other and not necessarily with any other arbitrary matrices).\n",
    "\n",
    "We can write some code to check whether the FDCT matrix multiplied by the IDCT matrix indeed gives us an identity matrix (as well as the other way around):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo_dct_identity():\n",
    "    idct_basis_np = np.array(idct_basis)\n",
    "    fdct_basis_np = np.array(fdct_basis)\n",
    "    \n",
    "    print(f\"IDCT * FDCT is: {idct_basis_np @ fdct_basis_np}\")\n",
    "    plt.pcolormesh(idct_basis_np @ fdct_basis_np)\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"FDCT * IDCT is: {fdct_basis_np @ idct_basis_np}\")\n",
    "    plt.pcolormesh(fdct_basis_np @ idct_basis_np)\n",
    "    plt.show()\n",
    "demo_dct_identity()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Annoyingly, we don't get back _exactly_ the identity matrix due to tiny rounding errors (on the order of $10^{-16}$), but the plots should help to show that the result is indeed visually very close to the identity matrix.\n",
    "\n",
    "### Orthonormal matrices\n",
    "\n",
    "The DCT matrices actually _even more_ special than just being invertible -- they are also _orthonormal_ matrices.\n",
    "\n",
    "One way to determine if a matrix is orthonormal is to check whether the inverse of the matrix is equal to its _transpose_ (a matrix with the rows and columns swapped). (Another property of matrices is that inverses, if they exist, must be unique. This implies that this property is a biconditional and works both ways.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo_idct_fdct():\n",
    "    idct_np_array = np.array(idct_basis)\n",
    "    fdct_np_array = np.array(fdct_basis)\n",
    "    difference = idct_np_array.T - fdct_np_array\n",
    "    print(difference)\n",
    "demo_idct_fdct()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of checking this with a math-y formula, we could have also shown this property by just looking at the code for the `compute_idct_basis` and `compute_fdct_basis` and noting just how similar they are (differing only in some `uv` and `xy` swaps).\n",
    "\n",
    "Now that we have explored some of these linear algebra properties, a question that might be asked is... can we use _other_ invertible matrices? The result won't be compatible with JPEG, but... _can we_, at least according to the mathematics? The answer is... yes!\n",
    "\n",
    "### Substituting the DCT with something else\n",
    "\n",
    "Let's suppose we are, um, an Independent Photographic Novice Hacker and we want to replace the JPEG DCT step with a different transform. One immediate obvious choice is to use the identity matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdct_basis = np.identity(64)\n",
    "idct_basis = np.identity(64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# demo this with Annex K 50% quality quantization tables\n",
    "encode_quant_tables = make_annex_k_example_quant_tables()\n",
    "to_encode_mcus = fdct_image(to_encode_y, to_encode_cb, to_encode_cr, to_encode_w)\n",
    "quantize_image(to_encode_mcus, encode_quant_tables)\n",
    "to_encode_toks = encode_misc_ops(to_encode_mcus)\n",
    "(encode_huff_tables, encode_huff_tables_for_dht) = compute_optimal_huffman_codes(to_encode_toks)\n",
    "to_encode_bitstream = encode_huffman(to_encode_toks, *encode_huff_tables)\n",
    "print(f\"Bitstream encoded to {len(to_encode_bitstream)} bytes\")\n",
    "our_encoded_jpeg = pack_final_jpeg(to_encode_bitstream, *to_encode_sz, encode_quant_tables, encode_huff_tables_for_dht)\n",
    "print(f\"Final compressed output is {len(our_encoded_jpeg)} bytes\")\n",
    "with open(\"ipnh_identity.jpg\", 'wb') as f:\n",
    "    f.write(our_encoded_jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm, that clearly didn't compress nearly as well as using the DCT, but we got _some_ output.\n",
    "\n",
    "If we try to pretend that our IPNH file is actually a JPEG, we get the following:\n",
    "\n",
    "![](ipnh_identity.jpg)\n",
    "\n",
    "And... huh. The human brain's ability to correct errors is quite impressive! What is happening here is that we have colors which have _not_ been transformed by the DCT, but a standard JPEG decoder is performing an IDCT on it anyways. This isn't very intuitively meaningful, but you can see hints of the DCT basis functions showing through.\n",
    "\n",
    "Let's decode the result back (again, using the identity transform rather than the IDCT):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ipnh_identity.jpg', 'rb') as f:\n",
    "    jpeg_data = f.read()\n",
    "parsed_jpeg = parse_jpeg_segments(jpeg_data)\n",
    "parsed_sof_sos_info = parse_sof_sos(parsed_jpeg)\n",
    "quant_tables = parse_dqt(parsed_jpeg)\n",
    "huff_tables = parse_dht(parsed_jpeg)\n",
    "decode_jpeg_mcus = decode_jpeg_scan(parsed_jpeg.scan_data, parsed_sof_sos_info, huff_tables[0], huff_tables[1])\n",
    "dequantize_jpeg(decode_jpeg_mcus, parsed_sof_sos_info, quant_tables)\n",
    "decoded_jpeg_components = idct_image(decode_jpeg_mcus, parsed_sof_sos_info)\n",
    "final_decoded_image = convert_color_space(decoded_jpeg_components, divroundup(parsed_sof_sos_info.x, 8) * 8)\n",
    "display(final_decoded_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can *clearly* see the 8x8 block structure in this image, as well as the effect of the quantization matrices affecting the colors in the upper-left corner differently compared to the rest of each block.\n",
    "\n",
    "Can we do better (worse)? Let's try an entirely random transform! In order to pick one, we can use SciPy to generate an orthonormal matrix with determinant 1 by asking for a member of the _special orthogonal group_ (a fancy name for the set of all matrices with this property)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdct_basis = sp.stats.special_ortho_group.rvs(64)\n",
    "idct_basis = fdct_basis.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# demo this with Annex K 50% quality quantization tables\n",
    "encode_quant_tables = make_annex_k_example_quant_tables()\n",
    "to_encode_mcus = fdct_image(to_encode_y, to_encode_cb, to_encode_cr, to_encode_w)\n",
    "quantize_image(to_encode_mcus, encode_quant_tables)\n",
    "to_encode_toks = encode_misc_ops(to_encode_mcus)\n",
    "(encode_huff_tables, encode_huff_tables_for_dht) = compute_optimal_huffman_codes(to_encode_toks)\n",
    "to_encode_bitstream = encode_huffman(to_encode_toks, *encode_huff_tables)\n",
    "print(f\"Bitstream encoded to {len(to_encode_bitstream)} bytes\")\n",
    "our_encoded_jpeg = pack_final_jpeg(to_encode_bitstream, *to_encode_sz, encode_quant_tables, encode_huff_tables_for_dht)\n",
    "print(f\"Final compressed output is {len(our_encoded_jpeg)} bytes\")\n",
    "with open(\"ipnh_random.jpg\", 'wb') as f:\n",
    "    f.write(our_encoded_jpeg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ipnh_random.jpg', 'rb') as f:\n",
    "    jpeg_data = f.read()\n",
    "parsed_jpeg = parse_jpeg_segments(jpeg_data)\n",
    "parsed_sof_sos_info = parse_sof_sos(parsed_jpeg)\n",
    "quant_tables = parse_dqt(parsed_jpeg)\n",
    "huff_tables = parse_dht(parsed_jpeg)\n",
    "decode_jpeg_mcus = decode_jpeg_scan(parsed_jpeg.scan_data, parsed_sof_sos_info, huff_tables[0], huff_tables[1])\n",
    "dequantize_jpeg(decode_jpeg_mcus, parsed_sof_sos_info, quant_tables)\n",
    "decoded_jpeg_components = idct_image(decode_jpeg_mcus, parsed_sof_sos_info)\n",
    "final_decoded_image = convert_color_space(decoded_jpeg_components, divroundup(parsed_sof_sos_info.x, 8) * 8)\n",
    "display(final_decoded_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This *also* did not compress particularly well, but once again it _worked_ in the sense that we were able to decode the result back into something vaguely resembling the input. Because we have chosen a random transform, the quantization error vaguely looks like \"noise\" across the entire 8x8 block.\n",
    "\n",
    "Can we break the rules even harder? What if we use an invertible transform that isn't even orthonormal?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdct_basis = np.random.rand(64, 64)\n",
    "# XXX it is *extremely* unlikely that a random matrix isn't invertible\n",
    "# (so we don't bother to check)\n",
    "idct_basis = sp.linalg.inv(fdct_basis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# demo this with Annex K 50% quality quantization tables\n",
    "encode_quant_tables = make_annex_k_example_quant_tables()\n",
    "to_encode_mcus = fdct_image(to_encode_y, to_encode_cb, to_encode_cr, to_encode_w)\n",
    "quantize_image(to_encode_mcus, encode_quant_tables)\n",
    "to_encode_toks = encode_misc_ops(to_encode_mcus)\n",
    "(encode_huff_tables, encode_huff_tables_for_dht) = compute_optimal_huffman_codes(to_encode_toks)\n",
    "to_encode_bitstream = encode_huffman(to_encode_toks, *encode_huff_tables)\n",
    "print(f\"Bitstream encoded to {len(to_encode_bitstream)} bytes\")\n",
    "our_encoded_jpeg = pack_final_jpeg(to_encode_bitstream, *to_encode_sz, encode_quant_tables, encode_huff_tables_for_dht)\n",
    "print(f\"Final compressed output is {len(our_encoded_jpeg)} bytes\")\n",
    "with open(\"ipnh_random_non_ortho.jpg\", 'wb') as f:\n",
    "    f.write(our_encoded_jpeg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ipnh_random_non_ortho.jpg', 'rb') as f:\n",
    "    jpeg_data = f.read()\n",
    "parsed_jpeg = parse_jpeg_segments(jpeg_data)\n",
    "parsed_sof_sos_info = parse_sof_sos(parsed_jpeg)\n",
    "quant_tables = parse_dqt(parsed_jpeg)\n",
    "huff_tables = parse_dht(parsed_jpeg)\n",
    "decode_jpeg_mcus = decode_jpeg_scan(parsed_jpeg.scan_data, parsed_sof_sos_info, huff_tables[0], huff_tables[1])\n",
    "dequantize_jpeg(decode_jpeg_mcus, parsed_sof_sos_info, quant_tables)\n",
    "decoded_jpeg_components = idct_image(decode_jpeg_mcus, parsed_sof_sos_info)\n",
    "final_decoded_image = convert_color_space(decoded_jpeg_components, divroundup(parsed_sof_sos_info.x, 8) * 8)\n",
    "display(final_decoded_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, it works even worse than before and yields even worse quality, but we *have* managed to get back something vaguely resembling the original input. Hopefully by this point you are convinced that _any_ invertible transform can (somewhat) be used.\n",
    "\n",
    "Knowing this, let's try an example that is once again a \"structured\" transform (just not a DCT) -- a [Hadamard transform](https://en.wikipedia.org/wiki/Hadamard_transform):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdct_basis = sp.linalg.hadamard(64) / 8\n",
    "idct_basis = fdct_basis.T\n",
    "\n",
    "demo_show_idct_basis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Hadamard transform looks \"vaguely like\" a DCT, except that it uses square waves (alternating between only two distinct values) and is rearranged in a slightly different order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# demo this with Annex K 50% quality quantization tables\n",
    "encode_quant_tables = make_annex_k_example_quant_tables()\n",
    "to_encode_mcus = fdct_image(to_encode_y, to_encode_cb, to_encode_cr, to_encode_w)\n",
    "quantize_image(to_encode_mcus, encode_quant_tables)\n",
    "to_encode_toks = encode_misc_ops(to_encode_mcus)\n",
    "(encode_huff_tables, encode_huff_tables_for_dht) = compute_optimal_huffman_codes(to_encode_toks)\n",
    "to_encode_bitstream = encode_huffman(to_encode_toks, *encode_huff_tables)\n",
    "print(f\"Bitstream encoded to {len(to_encode_bitstream)} bytes\")\n",
    "our_encoded_jpeg = pack_final_jpeg(to_encode_bitstream, *to_encode_sz, encode_quant_tables, encode_huff_tables_for_dht)\n",
    "print(f\"Final compressed output is {len(our_encoded_jpeg)} bytes\")\n",
    "with open(\"ipnh_hadamard.jpg\", 'wb') as f:\n",
    "    f.write(our_encoded_jpeg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ipnh_hadamard.jpg', 'rb') as f:\n",
    "    jpeg_data = f.read()\n",
    "parsed_jpeg = parse_jpeg_segments(jpeg_data)\n",
    "parsed_sof_sos_info = parse_sof_sos(parsed_jpeg)\n",
    "quant_tables = parse_dqt(parsed_jpeg)\n",
    "huff_tables = parse_dht(parsed_jpeg)\n",
    "decode_jpeg_mcus = decode_jpeg_scan(parsed_jpeg.scan_data, parsed_sof_sos_info, huff_tables[0], huff_tables[1])\n",
    "dequantize_jpeg(decode_jpeg_mcus, parsed_sof_sos_info, quant_tables)\n",
    "decoded_jpeg_components = idct_image(decode_jpeg_mcus, parsed_sof_sos_info)\n",
    "final_decoded_image = convert_color_space(decoded_jpeg_components, divroundup(parsed_sof_sos_info.x, 8) * 8)\n",
    "display(final_decoded_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This \"structured\" transform gives a size result that isn't quite as good as using the DCT and with subjectively slightly worse compression artifacts, but it's much better than the examples using random matrices.\n",
    "\n",
    "A natural question to ask after this observation is \"are there yet more ways to come up with some kind of 'structured' transform, possibly one which works even better?\" In other words, we want to find some transform which is, by some metric, \"most similar to\" or \"most aligned with\" the input image data. If we rummage around various \"linear algebra techniques from academia,\" one technique that pops out is [principal component analysis](https://en.wikipedia.org/wiki/Principal_component_analysis).\n",
    "\n",
    "PCA is a data processing technique that takes some input data (often something like \"outputs from running an experiment\") and finds the _principal components_ which best fit the data. These components line up with the _variance_ in the data. In the following demo, we treat all of the 8x8 block in the original image as one \"result of an experiment\" and try to find the components which \"explain\" most of the input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo_pca_transform():\n",
    "    global fdct_basis\n",
    "    global idct_basis\n",
    "    (input_w, input_h) = to_encode_sz\n",
    "\n",
    "    # shuffle each 8x8 block into a 64-element list, and accumulate them\n",
    "    X = []\n",
    "    def get_mcu_block(input, xbase, ybase):\n",
    "        output = []\n",
    "        for y in range(8):\n",
    "            for x in range(8):\n",
    "                px = input[(ybase + y) * input_w + (xbase + x)]\n",
    "                output.append(px)\n",
    "        return output\n",
    "    def blocks_for_component(input):\n",
    "        mcu_w = divroundup(input_w, 8)\n",
    "        mcu_h = divroundup(input_h, 8)\n",
    "        blocks = []\n",
    "        for y_mcu in range(mcu_h):\n",
    "            for x_mcu in range(mcu_w):\n",
    "                blocks.append(get_mcu_block(input, x_mcu * 8, y_mcu * 8))\n",
    "        return blocks\n",
    "\n",
    "    X += blocks_for_component(to_encode_y)\n",
    "    X += blocks_for_component(to_encode_cb)\n",
    "    X += blocks_for_component(to_encode_cr)\n",
    "    \n",
    "    # do the PCA\n",
    "    pca = PCA(n_components=64)\n",
    "    pca.fit(np.array(X))\n",
    "\n",
    "    # get the result\n",
    "    pca_result = pca.components_\n",
    "    # shuffle things into zigzag order to fit quantization better\n",
    "    idct_shuffled = [None] * 64\n",
    "    for i in range(64):\n",
    "        basis_func = pca_result[i]\n",
    "        idct_shuffled[ZIGZAG_TO_IDCT_BASIS_IDX[i]] = list(basis_func)\n",
    "\n",
    "    # set the result\n",
    "    idct_basis = idct_shuffled\n",
    "    fdct_basis = np.array(idct_basis).T\n",
    "\n",
    "    # plot the result (IDCT only as it is more intuitive)\n",
    "    demo_show_idct_basis()\n",
    "demo_pca_transform()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the plots, we see some \"vague stripe and checkerboard\" patterns, especially in the upper-left. Unlike the DCT, they aren't aligned with the x/y axes but are all slightly diagonal. Intuitively, this makes some sense -- the image does contain diagonal curves.\n",
    "\n",
    "However, theory is one thing. How well does this actually work?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# demo this with Annex K 50% quality quantization tables\n",
    "encode_quant_tables = make_annex_k_example_quant_tables()\n",
    "to_encode_mcus = fdct_image(to_encode_y, to_encode_cb, to_encode_cr, to_encode_w)\n",
    "quantize_image(to_encode_mcus, encode_quant_tables)\n",
    "to_encode_toks = encode_misc_ops(to_encode_mcus)\n",
    "(encode_huff_tables, encode_huff_tables_for_dht) = compute_optimal_huffman_codes(to_encode_toks)\n",
    "to_encode_bitstream = encode_huffman(to_encode_toks, *encode_huff_tables)\n",
    "print(f\"Bitstream encoded to {len(to_encode_bitstream)} bytes\")\n",
    "our_encoded_jpeg = pack_final_jpeg(to_encode_bitstream, *to_encode_sz, encode_quant_tables, encode_huff_tables_for_dht)\n",
    "print(f\"Final compressed output is {len(our_encoded_jpeg)} bytes\")\n",
    "with open(\"ipnh_pca.jpg\", 'wb') as f:\n",
    "    f.write(our_encoded_jpeg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ipnh_pca.jpg', 'rb') as f:\n",
    "    jpeg_data = f.read()\n",
    "parsed_jpeg = parse_jpeg_segments(jpeg_data)\n",
    "parsed_sof_sos_info = parse_sof_sos(parsed_jpeg)\n",
    "quant_tables = parse_dqt(parsed_jpeg)\n",
    "huff_tables = parse_dht(parsed_jpeg)\n",
    "decode_jpeg_mcus = decode_jpeg_scan(parsed_jpeg.scan_data, parsed_sof_sos_info, huff_tables[0], huff_tables[1])\n",
    "dequantize_jpeg(decode_jpeg_mcus, parsed_sof_sos_info, quant_tables)\n",
    "decoded_jpeg_components = idct_image(decode_jpeg_mcus, parsed_sof_sos_info)\n",
    "final_decoded_image = convert_color_space(decoded_jpeg_components, divroundup(parsed_sof_sos_info.x, 8) * 8)\n",
    "display(final_decoded_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dang. Not bad, but worse than the Hadamard transform, and we _continue_ to be unable to beat the DCT. Understanding _why_ we are struggling requires a different perspective on the DCT beyond pure linear algebra."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The DCT and spatial frequencies\n",
    "\n",
    "The DCT is often described as a \"Fourier-related\" transform that converts between the spatial (or time) domain and the frequency domain. But what does this actually mean?\n",
    "\n",
    "\"Fourier-related\" transforms are an entire family of operations which break down input into sinusoidal (sine, cosine, or some combination of both) functions. Some of them work on abstract and/or idealized mathematical functions whereas others are much more computation-friendly. Different transforms also make different assumptions about and have different restrictions on their input.\n",
    "\n",
    "Why do we care so much about sinusoid functions? Sinusoid functions often arise naturally in physical phenomena because of their association with movement along a circle, and because the _derivative_ and _integral_ of a sinusoid remains a sinusoid. Sinusoid functions also have a unique property that _linear combinations_ of sinusoids of a given frequency always result in a sinusoid of the same frequency. This \"non-mixing\" of frequencies can greatly simplify the mathematical modeling of physical phenomena.\n",
    "\n",
    "### JPEG and the DCT\n",
    "\n",
    "JPEG performs a 2-dimensional DCT on 8x8 blocks. A 2-dimensional transform is generated by multiplying sinusoids aligned with the $x$ axis by sinusoids aligned with the $y$ axis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo_2d_freqs():\n",
    "    one_d_freqs = []\n",
    "    for freq in range(8):\n",
    "        this_freq_wave = []\n",
    "        for x in range(8):\n",
    "            y = cos(pi*freq/8*(x+0.5))\n",
    "            this_freq_wave.append(y)\n",
    "        one_d_freqs.append(this_freq_wave)\n",
    "\n",
    "    fig, axs = plt.subplots(9, 9)\n",
    "    # plot along x\n",
    "    for freq in range(8):\n",
    "        axs[0, freq+1].pcolormesh([one_d_freqs[freq]])\n",
    "    # plot along y\n",
    "    for freq in range(8):\n",
    "        axs[freq+1, 0].pcolormesh(np.array([one_d_freqs[freq]]).T)\n",
    "    # plot products\n",
    "    for freq_x in range(8):\n",
    "        for freq_y in range(8):\n",
    "            freq_x_wave = np.array([one_d_freqs[freq_x]])\n",
    "            freq_y_wave = np.array([one_d_freqs[freq_y]])\n",
    "            freq_product = freq_x_wave.T @ freq_y_wave\n",
    "            axs[freq_y+1, freq_x+1].pcolormesh(freq_product)\n",
    "demo_2d_freqs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 8x8 block size was chosen as a tradeoff between speed (larger DCTs are more computationally expensive) and quality metrics. Larger DCTs do get used in more complex formats.\n",
    "\n",
    "For JPEG and lossy image compression in general, the DCT is very useful because the human visual system tends to be more sensitive to gradual changes (lower spatial frequencies) and less sensitive to rapid changes and fine detail (higher spatial frequencies). In the above plots, lower frequencies are towards the top-left and higher frequencies are towards the bottom-right.\n",
    "\n",
    "This (_subjective_!) behavior of the human visual system is why the example JPEG quantization table contains smaller numbers in the upper-left and larger numbers (throwing away more data) in the lower-right -- the data which is thrown away is data which the eyes are less sensitive to. This is also why JPEG arranges the coefficients in a zig-zag order -- it groups the frequences from lower (and thus more visible) to higher.\n",
    "\n",
    "One very notable entry in the above plots is the one exactly in the upper-left. Because $\\cos(0)=1$, that entry contains a constant value across the entire 8x8 block. When decomposing the source data into frequencies, this particular $(0, 0)$ frequency will contribute evenly across the source block, and so the amount of it which needs be added will be the average pixel value across the block. This property explains why the \"quick-and-dirty\" check of the decode (before any IDCT computations were done) works -- using only the average of each 8x8 block is a simple way to downscale the image by a factor of 8. This particular frequency gets referred to as the _DC coefficient_ because a frequency of 0 does not change and stays a fixed value, just like the voltage in a _direct current_ electrical circuit. The other coefficients are _AC coefficients_ because they vary sinusoidally like _alternating current_.\n",
    "\n",
    "The *entire* combination of breaking the image into frequencies, throwing away more of the high-frequency information, getting 0s out as a result of this throwing-away step, and finally the lossless compression results in (to recycle some ancient buzzwords) synergy! This is why hacking on only the DCT without fine-tuning everything else did not yield any improvements!\n",
    "\n",
    "For an example of a principled and properly-engineered lossy compression algorithm which _does_ improve upon JPEG, consider looking into formats such as [JPEG XL](https://en.wikipedia.org/wiki/JPEG_XL).\n",
    "\n",
    "### Why a _cosine_ transform?\n",
    "\n",
    "The discrete _Fourier_ transform decomposes the input data into a combination of both sine *and* cosine functions. This is usually represented by using complex numbers. However, if the input is an _even function_ (a function which is symmetric around the y axis), it can be represented using _only_ cosines (and so we won't need complex numbers). Is there a way to make sure our data is always an even function? What does it even mean for _discrete_ data consisting of a _finite_ set of numbers to be an even function?\n",
    "\n",
    "These discrete transforms handle this by making an _assumption_ that the input data repeats endlessly. Exactly _how_ it repeats corresponds to different transforms. For example, repeating end-to-end corresponds to the discrete Fourier transform. The discrete cosine transform instead repeats _mirrored_ copies of the data:\n",
    "\n",
    "<img src=\"imgs/dct.png\" width=\"500\" alt=\"Example of DCT mirroring when repeating data\"/>\n",
    "\n",
    "<small>Originally created by English Wikipedia user Stevenj</small>\n",
    "\n",
    "This repetition results in a periodic, infinite-length, even function.\n",
    "\n",
    "This _assumption_ which is being made implicitly is the reason why different choices of JPEG input padding (when the input is not a multiple of 8 pixels in width/height) matter -- this padding is mixed with the real image data and affects what frequencies come out.\n",
    "\n",
    "### Advanced -- sampling\n",
    "\n",
    "In the above demo, we used 8 equally-spaced frequencies. But how did we arrive at this? Can't frequencies theoretically be any real number?\n",
    "\n",
    "First of all, when something which is continuous gets sampled and turned into discrete data, there is an upper limit on \"useful\" frequencies. Anything above this cannot be captured accurately. For example, this includes very tiny objects which end up smaller than individual pixels on a camera's sensor. The precise limit is given by the [Nyquist-Shannon sampling theorem](https://en.wikipedia.org/wiki/Nyquist%E2%80%93Shannon_sampling_theorem) which states that the sampling frequency must be at least twice the frequency of the data you want to accurately reproduce. Data which is above this limit becomes indistinguishable from data at a lower frequency (an _alias_ frequency). In photography, this can result in artifacts such as [moiré](https://en.wikipedia.org/wiki/Moir%C3%A9_pattern).\n",
    "\n",
    "Between 0 and this upper limit, the \"proper\" frequencies to use is to space out the frequencies evenly when the input data consists of evenly-spaced samples in space (i.e. a grid of pixels, each of which is a sample, with equal spacing between the pixels i.e. a 1:1 pixel aspect ratio). This can be derived from the (continuous) Fourier transform via manipulations involving multiplying by a \"train\" of [Dirac delta functions](https://en.wikipedia.org/wiki/Dirac_delta_function), which is once again a typical homework problem in a university-level signals and systems lecture.\n",
    "\n",
    "Using frequencies which are _not_ evenly spaced corresponds to nonuniform sampling in the time/spatial domain. [Non-uniform discrete Fourier transforms](https://en.wikipedia.org/wiki/Non-uniform_discrete_Fourier_transform) can be used in these situations.\n",
    "\n",
    "### Advanced -- continuous functions\n",
    "\n",
    "All of the theory described so far has been focused on discrete data as that is the type of data which can be easily represented by a computer. The theory for _continuous_ Fourier-like transforms is significantly more complicated! Even most introductory textbooks will skip over some of this and only explain how to deal with functions which are \"nice.\"\n",
    "\n",
    "For quite some time, Fourier transforms didn't actually sit on solid theoretical foundations! Frequency-domain methods were so useful that they were being used well before _mathematical analysis_ was developed enough to justify them to modern standards of rigor. (So don't feel bad if you run into difficulties trying to understand this!)\n",
    "\n",
    "The best way to learn about this would probably be from university-level coursework and associated materials."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "We built a bare-minimum JPEG encoder from the ground up and explored a lot of the mathematics and signal processing which underlies it. We attempted various \"improvements\" to the algorithm but were ultimately unsuccessful. Hopefully you learned something, especially if you learned enough to demystify other more advanced lossy compression techniques.\n",
    "\n",
    "Questions? Comments? Corrections? Feel free to send feedback!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "image_compression_infodump",
   "language": "python",
   "name": "image_compression_infodump"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
